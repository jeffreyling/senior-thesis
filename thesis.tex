\documentclass[11pt]{report}
\usepackage{common}
\usepackage[round]{natbib}
\usepackage[strings]{underscore}
\usepackage{url}
\usepackage{todonotes}
\usepackage[toc]{appendix}

\title{Document Summarization (draft)}
\author{Jeffrey Ling}
%\date{}
\begin{document}
\maketitle{}

\begin{abstract} % needs work
Standard sequence-to-sequence (seq2seq) attention models have seen great success in NLP, but do not scale well to tasks with long sequences. We propose a novel coarse-to-fine attention method to reduce the number of computations necessary in standard attention. By organizing the source sequence into a 2-dimensional image, we hierarchically apply attention, using a coarse mechanism for the first layer to select a row sequence, and a finer mechanism for the second soft attention layer. While the computation for training standard seq2seq models scales linearly with source sequence length, our method is invariant to length and thus can scale arbitrarily.

We evaluate our model on the CNN/Dailymail document summarization task.
\end{abstract}

\tableofcontents{}




\chapter{Introduction}

\section{Natural Language Processing}

Natural language processing is a field with a variety of interesting structured prediction problems. The essential goal of NLP is to build a model of language so that computers can automatically process substantial quantities of text --- a highly relevant problem in today's information age.

While humans have no trouble understanding and using language, even the simplest language tasks can be impossible for computers. Some classical NLP problems include part-of-speech tagging, parsing, language modeling, and machine translation. %%

It is informative to consider the history of machine translation. The first methods were rule-based linguistics systems, and the company SYSTRAN provided one of the first \citep{}.

In the 90s, statistical methods for NLP became increasingly popular. By considering language generation as a probabilistic process, one can collect statistics over large corpuses of data to automatically deduce the parameters of the model. In machine translation, this idea was used to great effect in \citet{Brown1993}, whose count-based methods form the core of state-of-the-art systems like Google Translate.

Recently, researchers found that deep learning works effectively for many NLP tasks. For the first time, neural networks were able learning structure and features from language almost completely from scratch \citep{Collobert2011}. % \cite{Bengio2003} language model
The success of neural methods has been adopted by Google Translate to build even better systems \citep{GoogleTranslate2016}. Research in applying deep learning to NLP is ongoing.

\section{Deep Learning}

The history of neural networks dates back to the perceptron \citep{Rosenblatt1958}, a simple model that assumes data can be linearly separated. Due to this strict requirement, the machine learning community dismissed the idea as impractical, and research was sidelined for most of the 20th century.

%While some work continued \citet{Rumelhart1986} showed that the backpropagation algorithm could be used to efficiently train neural networks, 

Recently, neural networks have made a resurgence. In the ImageNet image classification competition in 2012, \citet{Krizhevsky2012} won using \emph{convolutional neural networks} \citep{LeCun1995}, beating the competition by a significant margin. This led to a renewed wave of research, especially due to the advancement of modern computing power and GPUs, which can train networks at 10 or 20 times the speed of standard CPUs. Today, deep models are used to successfully play Go \citep{Silver2016}, play Atari games from pixels \citep{deepatari2015}, and %???

%%% break

While neural networks are often treated as black box classifiers, \citet{Zeiler2014} show that the intermediate layers of deep convolutional networks contain abstracted qualities of the input, such as patterns, textures, and objects of the input. This suggests that neural networks are discovering features of the input and building generalized \emph{representations} of their inputs.

% image?

The idea of learning representations of the input is highly general, and so it comes as no surprise that deep networks soon found applications in NLP. \citet{mikolov2013word2vec} show that by training a neural network on a Google News text corpus, the network learned to map words in the English language to a vector of real numbers known as \emph{word embeddings}. These word embeddings are actually able to capture semantic properties of the words --- for example, taking the vectors for \emph{king}, \emph{man}, and \emph{woman}, we find that $v_{king} - v_{man} + v_{woman} \approx v_{queen}$, preserving the analogy that we usually make with text.

% diagram of vectors

Since the onset of deep learning, deep models have found their way into nearly every corner of NLP. Much of their success relies on the ubiquity of the \emph{long short-term memory} (LSTM) recurrent neural network \citep{hochreiter1997long}, a model used to both process and generate sequences of text. State-of-the-art systems for many problems, including machine translation, now use deep learning as their core algorithm. %???

% While there is much to cover  \cite{goldberg2015primer} provides a nice summary.


\section{Motivation}

% should this go under deep learning?
% this could be a hard section to write

Why deep models? There is a debate in the deep learning community about the extent to which human inductive biases ought to be incorporated into deep networks. On one hand, machine learning systems often incorporate handcrafted features or assumptions in the model to solve the problem. Examples % references? c

On the other hand, a purely data-driven system forgoes these details, using highly general models for the task at hand.

In the past, NLP was done with handcrafted features. However, \citet{Collobert2011} shows that this is an unnecessary assumption. They build a neural network for classical NLP tasks (e.g. part-of-speech tagging, named entity recognition) that can learn to do the task without any feature extraction.


There are a few reasons why deep networks are desirable for NLP. First, they are remarkably powerful and practically easier to train than complicated rule-based systems (assuming enough computational power is available), and are not mutually exclusive with standard feature extraction methods.
Second, they allow for non-domain experts to make progress on relevant problems.
Third, we find that trained models can discover latent structure in language automatically, which is an interesting phenomenon that may reveal more about how language is used. As an example, sequence-to-sequence models with attention \citep{bahdanau2014neural} learn the concept of a word alignment in translation without any supervision.

It is not yet clear, however, to what extent neural methods can replace models with hard-coded linguistic assumptions. This is one of the questions we set out to resolve.


\section{Our problem}

Text summarization is an important problem for compressing large bodies of natural text into a more easily digestable form. Document summarization is one of the most challenging formulations of this problem, where given a document with several sentences of text, the goal is to produce a coherent summary that captures most or all of its salient points.

To accomplish this, we use the most recent advances in deep learning to automatically produce summaries by training on a large dataset of given examples.

We can frame summarization as a supervised sequence-to-sequence task: given a news article (the source sequence of words), we desire to produce a summary (the target sequence). Existing methods in deep learning have been developed and proven to be highly effective for this kind of task, especially the sequence-to-sequence (seq2seq) model applied to machine translation \citep{sutskever2014sequence, bahdanau2014neural}. \citet{rush2015neural} uses the seq2seq model to summarize sentences into shorter headlines, but we consider the more general problem of arbitrarily long documents.

Existing seq2seq methods are limited by the length of source and target sequences. For a problem such as document summarization, the source sequence of length $N$ requires $O(N)$ seq2seq model computations. However, it makes sense intuitively that not every word of the document will be necessary for generating a summary, and so we would like to reduce the amount of necessary computations over the source document.

Therefore, in order to scale seq2seq methods for this problem, we aim to prune down the length of the source sequence in an intelligent way. In the problem of image captioning, \citet{xu2015captioning} show that when generating words of the caption, the model places attention on the relevant part of the image.
Inspired by this idea, we introduce the coarse-to-fine attention mechanism that sparsely selects subsets of the source document for processing.


We provide an outline for the rest of this thesis.
In section ? we cover related work in summarization and deep learning.
In section ? we give the necessary background for our models.
In section ? we describe our models in detail.
In section ? we show results and discussion.



%%%%%%%%%%

\chapter{Related Work}

\section{Automatic Summarization}

\citet{Nenkova2011} give an overview of the field. In particular, they provide a taxonomy of methods that researchers have used to frame summarization:

\paragraph{Extractive vs. Abstractive} Extractive summaries extract certain sentences or phrases from the document, while abstractive summaries take a more holistic view and can in practice be anything (similar to how humans produce summaries). 

\paragraph{Single- vs. Multi-document} Summarization was originally posed as the problem of producing a summary for a single document. However, with the onset of the Internet and search engines, there are often multiple documents on the same topic, and so a summarization system should be able to use all of them to produce a summary.

\paragraph{Indicative (style) vs. informative (facts)} Indicative summaries provide a sense for what a certain document is about without necessarily giving all of its details, while informative summaries are meant to replace the original document in terms of content.

\paragraph{Keyword (words) vs. headline (sentence)} Keyword summaries are allowed to simply be a bag-of-words of important keywords from the document, while headline summaries must form a coherent sentence.

\paragraph{Generic vs. query focused} Generic summaries have no assumptions on the reader and are meant to be generally informative, while query focused summaries take into consideration a query and only return relevant information. The contrast between these two methods highlights an important question in summarization: to what end are we summarizing documents? If we can answer this question more precisely, we will be better able to build systems to accomplish our desired tasks.

\vspace{0.5cm}

Before elaborating on specific methods, it is important to highlight these differences to understand how people thought about the problem. For example, extractive methods are by far more popular due to the simplicity of building an extraction algorithm, while abstractive methods have been difficult as we do not have a fully working model of language generation.

Based on this taxonomy, the deep model we set out to build would be classified as 1) abstractive, 2) single-document, 3) informative, 4) headline, 5) generic.

In the rest of this thesis, we will limit the scope of the summarization problem to the \emph{single-document}, \emph{informative}, \emph{headline}, and \emph{generic} categories in order to focus on the application of deep learning in the problem.

In the next section, we give a brief overview of some of the relevant methods used in summarization.

\section{Methods}



\subsection{Classical}

One of the first considerations of automatically producing summaries was \cite{luhn1958automatic}, which aimed to summarize scientific articles using a sentence-ranking method. The algorithm gives each sentence a score based on the occurrence of frequently appearing words.


Since then, a variety of approaches have been used to solve summarization. We highlight some notable work in both the extractive and abstractive framework.

\subsubsection{Extractive}

The most popular methods for document summarization have generally been extractive due to their simplicity. One natural procedure for an extractive summarization is to score sentences based on some relevance metric and return the highest scoring sentences (perhaps reordering them as well).

Some examples are \cite{carbonell1998MMR}, which uses a simple information metric for ranking sentences, and \cite{svore2007}, which uses a simple neural network for the same purpose.

\citet{Shen2004} models sentence extraction as a sequential decision problem, using a linear-chain conditional random field to find the best subset of sentences.



\subsubsection{Abstractive}

While extraction has proven to be successful, the method is inherently limited in its ability to summarize. The more challenging method, and also the closest to what humans do, is \emph{abstractive} summarization. Instead of strictly requiring that all words of the summary come from the source document, any coherent text is allowed.

Two methods used to produce abstractive summaries are sentence compression and sentence fusion. Compression removes less useful information from sentences, while fusion is harder and combines information from sentences. % cite Barzilay, MCkeown

Compression: \citet{knight2002summarization} employs a noisy channel model, similar to machine translation, to deduce the ``most probable'' compression, while \citet{clarke2008global} uses an integer linear program. \citet{cohn2008sentence} extend the tree-based methods to allow for insertions and substitutions during compression, whereas prior methods were purely deletion based. \cite{zajic2004topiary} successfully use a sentence compression algorithm along with an unsupervised topic model on the DUC 2004 task.

Fusion: align parse trees and combine phrases that are similar \todo{finish}


\subsection{Deep Learning}

With the onset of deep learning, learning an end-to-end abstractive model for summarization has become more feasible. \citet{rush2015neural} propose a data-driven, completely abstractive model for summarizing short sentences by training a sequence-to-sequence model with attention. More recent work in deep learning has been done for both extractive \citep{} %% citation needed
and abstractive \citep{nallapati2016seq2seq, ramachandran2016} methods that scale the models to full documents, demonstrating the feasibility of end-to-end models.

These new models require a large amount of supervised training data, which the DUC data are unsuitable for due to their small scale. However, thanks to the large-scale annotated CNN/Dailymail news stories dataset \cite{Hermann2015}, we now have the necessary data to train our deep models.

The task has not yet been fully standardized in this context, and research in the area is still largely preliminary. While CNN/Dailymail may not be the most suitable dataset for the task due to its noisiness \citep{Chen2016}, a better alternative is yet to exist.


\subsection{Datasets}

To standardize the task, NIST released data for DUC (Document Understanding Conferences) between 2001-2007 \citep{over2007duc}. The DUC tasks involved producing summaries for both single- and multiple-document sets of news articles. DUC 2001 and 2002 ask for general summaries of these articles documents and summaries, while DUC 2003-2006 also evaluate summaries based on their usefulness for certain question-answer tasks. % more details?

While a single most effective metric for summarization may not exist, the DUC conferences established several important criteria, including grammaticality, non-redundancy, and content coverage (for which metrics like ROUGE \citep{lin2004rouge} were created).

DUC overall was found not to have the best impact. For many of the news summary tasks, it was found that taking the first sentence of each article could not be beaten by more sophisticated methods, and so the task was somewhat abandoned.

CNN/Dailymail is much larger, and is the one we use to train our deep models.


\subsection{Evaluation}

Evaluating a good summary is inherently ambiguous, and probably one of the hardest parts of the problem.

For extractive summaries, people have proposed simple metrics such as precision and recall on selected sentences. These naturally do not work too well since 1) not all sentences are equally informative, and 2) not all parts of a sentence are relevant.

DUC really pushed forward understanding on evaluation. They came up with recall on elementary discourse units (EDUs), based on clauses within a summary that ought to be captured. ROUGE \cite{lin2004rouge} is cheap and fast. Pyramid method is a complicated human evaluation method based on summary content units (SCUs). 

None of these methods directly address the grammaticality of the output. Aside from using human evaluation, meaningful metrics for summaries is still very much an open question \citep{toutanova2016summarymetrics}. In our work, we settle for ROUGE due to its cheapness and ease of use in evaluating our models.


\subsection{In the wild}

Summarization is an important real-world problem due to the explosion of available data. Thus, there are many practical methods that have been developed and deployed in real-world settings. One of the most notable examples is on Reddit\footnote{\url{reddit.com}}: in order to summarize long forum discussions, Reddit uses technology from \textsc{smmry}\footnote{\url{smmry.com}}.

Smmry's algorithm is a simple extractive summarization method. It counts word occurrences, splits discussions by sentence, and ranks the sentences based on the sum of their word scores (perhaps tf-idf?). This algorithm bears extraordinary similarity to \citet{luhn1958automatic} --- although a variety of methods have been invented since then, the simplest approaches turn out to be the most practical.

% Other examples include a SUMMLY app thing acquired by Yahoo - it had a lot of hype but it's unclear if it had any real content.


%%%%%%%
\chapter{Background}

In this chapter, we set up the relevant background ideas for our models.

\section{Sequence-to-Sequence Attention Models}

The sequence-to-sequence architecture \citep{sutskever2014sequence}, also known as the encoder-decoder architecture, forms the backbone of many successful models in NLP. A popular variant of sequence-to-sequence models are \emph{attention} models \citep{bahdanau2014neural}. The key idea is to keep an encoded representation of all parts of the input, attending to the relevant part each time we produce an output from the decoder.
These models have been used to great effect in a variety of NLP tasks, including machine translation \citep{sutskever2014sequence, bahdanau2014neural}, question answering \citep{Hermann2015}, dialogue \citep{li2016persona}, caption generation \citep{xu2015captioning}, and in particular summarization \citep{rush2015neural}. In particular these works show that learning an accurate attention function is important for good performance. %%???

\citet{xu2015captioning} show how attention models can be used to ``summarize'' an image and produce a caption. 
By analyzing where in the image their models attend to when generating each word of the caption, they qualitatively find that the model is essentially describing that region of the image. 
Figure~\ref{} shows some examples.

% image?

We can leverage the same idea for text summarization, assuming we have a suitable representation of our input document. The simplest method for doing so would be to run an LSTM over the document.

However, the attention step becomes computationally difficult --- for each word we generate, we need to compare it to every word of the document in order to determine which part to attend to. Therefore, we propose a hierarchical method of attending to the document by first attending to sentences, then to the words within sentences. We call this method \emph{coarse-to-fine attention}
\footnote{The term coarse-to-fine attention has previously been introduced in the literature \citep{mei2016}. However, their idea is different: they use coarse attention to reweight the fine attention computed over the entire input. Similar ideas have also been called hierarchical attention \citep{nallapati2016seq2seq}.}.



To be able to attend to both sentences and words in a hierarchical manner, we need to construct encodings of the document at both levels. Thus, we run a low-level LSTM encoder on the words of each sentence for a fine-grained representation of the text, and a simpler encoder model (e.g. bag of words) for coarse-grained sentence representations.
\citet{Sukhbaatar2015} demonstrate how coarse representations can be useful by using memory networks to access information for simple question-answering tasks.
\citet{li2015autoencoder} use the idea of coarse and fine encodings to develop a hierarchical autoencoder for representing paragraphs of text.
%by using low-level LSTMs on words to build sentence representations, then a high-level LSTM on the sentences for the final paragraph representation.

Therefore, if we can make our model first use coarse attention to choose sentences, then use fine attention to choose words only from that sentence, then we avoid the computational cost of searching over the entire document. This idea runs into a very serious problem, however: by posing the attention as a discrete selection process, the neural network is no longer differentiable.


\citet{xu2015captioning} suggest ``hard'' attention as one possible solution to the discrete selection problem. While standard ``soft'' attention actually averages the representations of where the model attends to, for hard attention we make a hard decision and choose only one location. Such models can be trained using reinforcement learning. Before we elaborate on this method, we survey some other methods invented to overcome this discrete attention problem.

\section{Conditional Computation}


%The problem of full document summarization, however, is still very open. In order to make the models work, \cite{nallapati2016seq2seq} use a variety of tricks: limiting the decoder vocabulary to the document, the use of pointer attention for \texttt{<unk>} tokens, etc. Our goal in this paper is not necessarily to optimize performance, but to understand how to scale up existing seq2seq methods in an efficient way, and so we attempt to eliminate the use of these ad-hoc tricks whenever possible. We aim to find the best implementation of ``hard'' attention, in which a discrete subset of the source document is selected at any given time, to satisfy the scalability condition.

\todo{make this section more coherent}

Many techniques have been proposed in the literature to handle the problem of large inputs to deep neural networks.

The term ``conditional computation'' was coined by \citet{BengioLC13}, where the idea is to compute a subset of units for a given network per input. This would have the advantage of being much more efficient, especially for networks that handle extremely large inputs as is common in vision and NLP.

While standard deep models use the softmax function $\mathrm{softmax}(\boldz)_i = \exp(z_i) / \sum_j \exp(z_j)$ to produce differentiable probability distributions, \citet{martins2016sparsemax} propose the sparsemax function as a sparse alternative to softmax that still has a useful gradient.

\citet{rae2016sparsememory} use an approximate nearest neighbors approach for their ``sparse access memory'' model to train a large-scale neural Turing machine.

\citet{Shazeer2017} introduces a mixture-of-experts model that selectively chooses a subset of ``expert'' networks at any given time during training. In the spirit of conditional computation, they only train $K$ experts at a time using a sparse gating function.

These methods are all promising and merit further research in the future. In this work, we limit our scope to the hard attention model of \citet{xu2015captioning}. In the next section, we explain reinforcement learning and how it can be used to train the hard attention model.

\section{Reinforcement Learning}

Standard backpropagation training of neural networks assumes that the output is a deterministic and differentiable function of the input. Reinforcement learning, however, is a more general framework that makes no such assumptions.

The traditional setup of reinforcement learning (RL) assumes some agent is navigating an environment and earning rewards.
We assume a state space $\mcS$, an action space $\mcA$, a reward function of state and action $R : \mcS \times \mcA \to \reals$, and a transition distribution $p(s_{t+1} | s_t, a_t)$ (so that our environment is Markovian).

We suppose that at time $t$, the agent is in state $s_t \in \mcS$, makes an action $a_t \in \mcA$, earns a reward $r_t = R(s_t, a_t)$, and transitions probabilistically to the next state $s_{t+1}$.
Assuming the reward function is unknown, the agent wants to maximize total expected reward
$$\mathbb{E}_{s_t, r_t} [\sum_{t=0}^\infty \gamma^t r_t]$$
by finding an optimal action policy. Here, $\gamma$ is a time discount factor.

% diagram

\subsection{Methods for training RL agents}

A variety of methods have been invented for solving the RL problem, i.e. finding the optimal policy $\pi(s)$ for a given state $s$.

When the transition distribution $p(s_{t+1} | s_t, a_t)$ and reward function $R(s,a)$ is known, our problem is also known as a Markov decision process (MDP). We can compute an optimal value function $Q(s, a)$ that represents the estimated reward from taking action $a$ in state $s$:
\begin{align}
Q(s, a) & = R(s, a) + \gamma \sum_{s' \in \mcS} p(s' | s, a) V(s') \\
V(s) &= \max_{a \in \mcA} Q(s,a)
\end{align}
for every $s \in \mcS, a \in \mcA$. These equations are known as the Bellman equations, and finding an optimal policy then amounts to solving this system for $\pi$. This can be done by treating the system as a fixed point problem and using iterative methods. % citation

When we don't know the transition distribution, we can estimate it by standard maximum-likelihood methods and exploration of the state space. The same iteration techniques then apply.

When we don't know the reward function \emph{and} we don't know the transition distribution, things become trickier. While the environment still gives us rewards for actions, finding the optimal policy requires knowing which states lead to those rewards.
Since we don't know ahead of time which states are best, we are forced to extensively explore the state space to find what actions lead to the best rewards.

There are two methods for approaching the general RL problem. First, model-based approaches attempt to estimate the transition distribution $p(s_{t+1} | s_t, a_t)$ and apply the Bellman equations to find the optimal policy. Second, model-free approaches forgo the transitions and simply learn what action is best in a given state.

\subsection{Model-free approaches}

We consider the model-free approach in more detail.

\paragraph{Q-learning} One technique is to model the Q-function $Q(s,a)$ that estimates the total reward of action $a$ in state $s$. To learn the Q-function, we predefine some policy based on our current estimates of the Q-function, and we make learning updates as
\begin{equation}
Q(s,a) \gets Q(s,a) + \eta \left(r + \gamma \max_{a' \in \mcA} Q(s', a') - Q(s,a) \right)
\end{equation}
upon taking action $a$ in state $s$, where $\eta$ is a learning rate, $r$ is the reward received, and $s'$ is the state we transition into. This is known as the \emph{Q-learning} update rule. An alternative update rule takes into account the action $a'$ we would take next in state $s'$:
\begin{equation}
Q(s,a) \gets Q(s,a) + \eta  \left(r + \gamma Q(s', a') - Q(s,a) \right)
\end{equation}
This is known as the SARSA update rule.

In both cases, if we have a large state space, we may not want to record $Q(s,a)$ for every pair of $s,a$. We can instead parametrize $Q(s,a)$ with weights $w$, and maximizing reward using backpropagation gives us the update rule
\begin{equation}
w \gets w + \eta \left(r + \gamma \max_{a' \in \mcA} Q(s', a') - Q(s,a) \right) \frac{\partial Q(s,a)}{\partial w}
\end{equation}

\paragraph{Policy gradient} An alternative to modeling the Q-function is to learn a policy $\pi : \mcS \to \mcA$ directly. We parametrize $\pi$ with weights $w$, and assume that at training time, we have a probability distribution $p(a|s ; w)$ over the actions that we sample from. Then assuming we get reward $r$, our update is
\begin{equation}
w \gets w + \eta \mathbb{E}_a \left[ r \cdot \frac{\partial \log p(a | s ; w)}{\partial w}  \right]
\end{equation}

This is known as the policy gradient update or REINFORCE algorithm \citep{williams1992reinforce}. In practice, the expectation is replaced by a single Monte Carlo sample. \todo{should I give the derivation?}

If we have a full trajectory of states $s_t$ and rewards $r_t$, then we can assume our action $a_t$ at time $t$ led to future discounted reward $R_t = \sum_{k=t}^\infty \gamma^{k-t} r_k$. We then have the update rule
\begin{equation}
w^{(t+1)} \gets w^{(t)} + \eta \mathbb{E}_{a_t} \left[ R_t \cdot \frac{\partial \log p(a_t | s_t ; w)}{\partial w}  \right]
\end{equation}

%\paragraph{TD-learning} An alternative to modeling the Q-function is to model the value function $V(s) = \max_{a \in \mcA Q(s,a)$, which can be a simpler learning problem. 
%
%Again, suppose we parametrize $V(s)$ with weights $w$, and suppose we have an observed sequence of states $s_t$ and rewards $r_t$. Instead of making updates to $V(s)$ independently for each time $t$, we can take into account our entire trajectory, giving the update rule at time $t$
%\begin{equation}
%w \gets w + \eta \left(r_t + \gamma V(s_{t+1}) - V(s_t) \right) \sum_{k=1}^t \lambda^{t-k} \frac{\partial V(s_k)}{\partial w}
%\end{equation}
%where $\lambda$ is a discount factor for past gradients.
%
%This update is known as the $TD(\lambda)$ update, and is a special case of temporal difference learning (or TD-learning). Taking into account the entire trajectory allows us to obtain more information about which past actions and states led to the best rewards. TD-learning was used to train a world-class backgammon bot \citep{Tesauro1994}.

%This raises two problems. First is the \textbf{credit assignment problem}: we don't know which actions we chose in the past led us to the best rewards. For example, if we are one move away from winning a game and earning a big reward, we don't assign the credit of the reward to that one move, but rather to some good moves we made in the past that led us to the winning state.
%Second is the \textbf{exploration-exploitation problem}. On one hand, we need to explore the state space to determine which states have the best rewards, but on the other hand, we are missing the opportunity to exploit the known high-reward actions.

%occasionally choosing random actions to better explore the state space (also known as an $\epsilon$-greedy exploration policy).


\section{Deep reinforcement learning}

Reinforcement learning and deep learning have successfully been combined to play Go \citep{Silver2016}, 
control robots \citep{Lillicrap2016}, and play Atari games from pixels \citep{deepatari2015}.

Aside from these classical applications of RL, the RL framework can be applied to train deep neural networks with stochastic units. We cast our learning problem with the neural network as an agent with a parametrized policy function $\pi : \mcS \to \mcA$, where the states $\mcS$ are the inputs and the actions $\mcA$ are the possible outputs of each stochastic unit. The rewards are then negative loss (e.g. from regression or classification).

More concretely, consider a sequential classification problem such as summarization, and consider the sequence-to-sequence model with attention. At each time step $t$ of the decoder, our state $s_t$ is given by the LSTM state and the encoded context, and our action $a_t$ is where in the encoder context to attend to. The reward $r_t$ is then the log-likelihood of predicting the gold word (according to some supervised dataset).

% diagram?

To obtain gradients for backpropagation, we can apply policy gradient methods for the nondifferentiable stochastic units. Because our network has both differentiable and nondifferentiable weights on the pathways to the output, we can consider it as a \emph{stochastic computation graph} \citep{schulman2015backprop}. To train the weights directly connected to the loss function, we backpropagate the standard gradient $\partial \mcL / \partial w$. To train weights that are not directly connected, but precede an intermediate stochastic unit with distribution $p(a | s ; w)$, we backpropagate the policy gradient $\partial \log p(a | s; w) / \partial w$.

% diagram?


\subsection{Deep RL in NLP}

In computer vision, deep reinforcement learning has been applied with some success 
\citep{mnih2014visualattention, ba2015visualattention, xu2015captioning}.

In NLP, RL has been attempted with varying degrees of success so far.
\citet{ranzato2015} 
\citet{li2016dialogueRL} uses RL to train a dialogue system.
\citet{narasimhan2016} uses RL to improve information extraction on scarce data.
\citet{Narasimhan2015} uses RL play text-based games.
%\citet{Andreas2016} uses RL to compose 
Most interestingly, \citet{Yogatama2017} uses RL along with an architecture called a tree LSTM that produces a tree latent variable on a sentence using \textsc{Shift} and \textsc{Reduce} actions. They show that the actions that the tree LSTM learn produce reasonable parse structures on the input sentences.


Inspired by these ideas, we aim to improve on previous work for document summarization by (1) implementing conditional computation using coarse-to-fine attention at the sentence level, and (2) including reinforcement learning for a sparse coarse attention layer.

In the next chapter we describe our models in detail.



\chapter{Models}

\section{Sequence-to-sequence (seq2seq)}

We first describe the neural network architecture of the seq2seq models, also known as encoder-decoder models \citep{bahdanau2014neural}.

\subsection{Recurrent encoder and decoder}

In the seq2seq model, an \emph{encoder} recurrent neural network (RNN) reads the source sequence as input to produce the \emph{context}, and a \emph{decoder} RNN generates the output sequence using the context as input.  One popular RNN choice is the long-short term memory (LSTM) network \citep{hochreiter1997long}.

% more specifics here?
More formally, suppose we have a vocabulary $\mcV$. A given input sentence $w_1, \ldots, w_n \in \mathcal{V}$ is transformed into a sequence of vectors $\boldx_1, \ldots, \boldx_n \in \mathbb{R}^{d_{in}}$ through a word embedding matrix $\boldE \in \mathbb{R}^{|\mathcal{V}| \times d_{in}}$ as $\boldx_t = \boldE w_t$.

An RNN is given by a parametrizable function $f_{enc}$ and a hidden state $\boldh_t \in \mathbb{R}^{ d_{hid}}$ at each time step $t$ with $\boldh_t = f_{enc}(\boldx_t, \boldh_{t-1})$. For the LSTM, we keep an auxiliary state $\boldc_t$ along with $\boldh_t$, and we compute $f_{enc}$ as
\begin{align}
\boldf_t &= \sigma(\boldW^f \boldx_t + \boldU^f \boldh_t + b_f) \\
\boldi_t &= \sigma(\boldW^i \boldx_t + \boldU^i \boldh_t + b_i) \\
\boldo_t &= \sigma(\boldW^o \boldx_t + \boldU^o \boldh_t + b_o) \\
\boldc_t &= \boldf_t \odot \boldc_{t-1}  + \boldi_t \odot \tanh(\boldW^c \boldx_t + \boldU^c \boldh_{t-1} + b_c) \\
\boldh_t &= \boldo_t \odot \tanh(\boldc_t)
\end{align}
where $\boldW, \boldU, b$ are learned parameters, and $\odot$ is the elementwise product. 
Intuitively, $\boldc_t$ is the memory cell, $\boldf_t$ is the forget gate, $\boldi_t$ is the input gate, $\boldh_t$ is the output cell, and $\boldo_t$ is the output gate.

LSTMs can be stacked on top of one another by treating the outputs $\boldh_t$ of one LSTM as the inputs to another LSTM.

In stacked LSTMs, we will take the top sequence of hidden states $\boldh_1, \ldots, \boldh_n$ to form a single context vector.

%% diagram

The decoder is another RNN $f_{dec}$ that generates output words $y_t \in \mathcal{V}$. It keeps hidden state $\boldh_t^{dec} \in \mathbb{R}^{d_{hid}}$ as $\boldh_t^{dec} = f_{dec}(y_{t-1}, \boldh_{t-1}^{dec})$ similar to the encoder RNN.
A context vector is produced at each time step using an attention function $a$ that takes the encoded hidden states $[\boldh_1, \ldots, \boldh_n]$ and the current decoder hidden state $\boldh_t^{dec}$ and produces the context $\boldc_t \in \reals^{d_{ctx}}$:
\begin{equation}
\boldc_t = a([\boldh_1, \ldots, \boldh_n], \boldh_t^{dec})
\end{equation}
As in \citet{luong2015effective}, it is helpful to feed the context vector at time $t-1$ back into the decoder RNN at time $t$, i.e. $\boldh_t^{dec} = f_{dec}([y_{t-1}, \boldc_t], \boldh_{t-1}^{dec})$.

Finally, a linear projection produces a distribution over output words $y_t \in \mcV$:
\begin{equation}
p(y_t | y_{t-1}, \ldots, y_1, [h_1, \ldots, h_n]) = \boldW^{out}\boldc_t + b^{out}
\end{equation}
The models are trained to maximize the log probability of getting the sequences in the dataset correct. As the model is fully differentiable with respect to its parameters, we can train it end-to-end with stochastic gradient descent and the backpropagation algorithm.

We note that we have great flexibility in how our attention function $a$ combines the encoder context and the current decoder hidden state. In the next few sections, we explain standard choices for $a$ as well as our proposed coarse-to-fine attention model.

\section{Model 0: Standard Attention}

In \cite{bahdanau2014neural}, the function $a$ is implemented with an \emph{attention network}. We compute attention weights for each encoder hidden state $h_i$ as follows:
\begin{align}
\beta_{t,i} &= \boldh_i^\top \boldW^{attn} \boldh_t^{dec} \quad \forall i = 1, \ldots, n\\
\balpha_t &= \mathrm{softmax}(\bbeta_t) \\
\widetilde{\boldc}_t &= \sum_{i=1}^n \alpha_{t,i} \boldh_i
\end{align}
The idea behind attention is to select the most relevant words of the source (by assigning higher attention weights) when generating output word $y_t$ at time $t$.

The $\mathrm{softmax}$ function, defined as
$$\mathrm{softmax}([\beta_1, \ldots, \beta_n])_i =  \frac{\exp(\beta_i)}{\sum_{j=1}^n \exp(\beta_j)}$$
normalizes the $\alpha_i$ to sum to 1 over the source sentence words. This gives us a notion of probability distribution over the encoder words --- we can therefore write $\boldc_t$ as the expectation $\mathbb{E}_\alpha[\boldh]$, where we pick $\boldh_i$ with probability $\alpha_i$.

Our final context vector is then
\begin{equation}
\boldc_t = \tanh(\boldW^2[\widetilde{\boldc}_t, \boldh_t^{dec}])
\end{equation}
for $\boldW^2 \in \reals^{2d_{hid} \times d_{ctx}}$ a learned matrix.

Going forward, we call this this instantiation of the attention function \textsc{Model 0}.


\section{Model 1 and 2: Coarse-to-Fine Soft Attention}

For a large source input like a document, it may be computationally inefficient to run an RNN over the entire source. Instead, we can consider organizing the document into distinct sentences and select one sentence to attend to at a time.

Specifically, suppose we have sentences $s_1, \ldots, s_m$ with words $w_{i,1}, \ldots, w_{i,n_i}$ for sentence $s_i$, so that we can apply an RNN to each separately to get corresponding hidden states $\boldh_{i,j}$ for $i = 1, \ldots, m$ and $j = 1, \ldots, n_i$.
For attention, we then consider two options.

\paragraph{Model 1} We can follow \textsc{Model 0} and compute attention weights $\alpha_{i,j}$ for each hidden state $h_{i,j}$ by normalizing over all states. We call this \textsc{Model 1}.

\paragraph{Model 2} Alternatively, rather than taking attention over the entire document, we can instead have a two-layered hierarchical attention mechanism: first, we have weights $\alpha_1^s, \ldots, \alpha_m^s$ for each sentence, and then for sentence $s_i$, we have another set of weights $\alpha_{i,1}^w, \ldots, \alpha_{i,n_i}^w$.
Our final attention weight on word $w_{i,j}$ is then $\alpha_{i,j} = \alpha_i^s \cdot \alpha_{i,j}^w$.

In order to compute the sentence attention weights $\alpha_i^s$, we need to produce representations of each sentence; i.e., given the words $w_{i,1}, \ldots, w_{i, n_i}$ of the sentence, we produce a vector representation $\boldh^s_i \in \mathbb{R}^{d_{sent}}$.

Our first option is bag of words: we simply take the representation as
\begin{equation}
\boldh_i^s = \sum_{j=1}^{n_i} \boldE w_{i,j}
\end{equation}
i.e. the sum of the word embeddings, where $\boldE$ is another embedding matrix.

Alternatively, we can use a convolutional method: as in \citet{kim2014convolutional}, we perform a convolution over each window of words in the sentence using a fixed kernel width. We use max-over-time pooling to obtain a fixed-dimensional sentence representation in $\mathbb{R}^{d_f}$ where $d_f$ is the number of filters.

Explicitly, fix a sentence and suppose we have word vectors $\boldx_1, \ldots, \boldx_n$ with $\boldx_j = \boldE w_j \in \reals^{d_{in}}$, and suppose we have kernel width $k$ and convolution weights $\boldW^{conv} \in \reals^{d_f \times d_{in} k}$ where $d_f$ is the number of filters. Then applying the convolution
$\boldW^{conv} * [\boldx_1, \ldots, \boldx_n]$ gives result $\boldu = [\boldu_1, \ldots, \boldu_{n-k+1}]$ with $j$th element
$$\boldu_j = \boldW^{conv} \cdot [\boldx_j, \boldx_{j+1}, \ldots, \boldx_{j+k-1}] \in \reals^{d_f}$$
Our final output is given by 
\begin{equation}
\boldh^s = \mathrm{max-over-time}(\tanh(\boldu + b^{conv}))
\end{equation}
where $\mathrm{max-over-time}$ takes the maximum along the $j$-indexing dimension.

% include convolution equations?

Thus, using the sentence representations, we can compute attention over the sentences and over the words in each sentence in the same way as \textsc{Model 0}. Using attention on word $w_{i,j}$ as  $\alpha_{i,j} = \alpha_i^s \cdot \alpha_{i,j}^w$, we can proceed exactly as in \textsc{Model 0} by computing the weighted average over hidden states $\boldh_{i,j}$.

We call this method of attention \textsc{Model 2}.

\section{Model 3: Coarse-to-Fine Sparse Attention}

With the previous models, we are required to compute hidden states over all words and sentences in the document, so that if we have $M$ sentences and $N$ words per sentence, the computational complexity is $O(MN)$ for each attention step.

However, if we are able to perform conditional computation and only compute on $M^+$ of the sentences, we can reduce the complexity to $O(M^+N)$. If we are able to make $M^+$ constant or even 1, this would give significant benefits to our overall complexity.

In our experiments, we will apply stochastic sampling to the attention distribution $\balpha$ in the spirit of \citet{xu2015captioning} and ``hard attention''.
Specifically, rather than computing the context $\widetilde{\boldc}$ as an expectation over $\balpha$ (i.e. $\boldc = \sum_{i=1}^n \alpha_i \boldh_i$), we can sample from the probability distribution $\balpha$ to obtain a single state $\boldh_i$, and we set $\widetilde{\boldc} = h_i$ as the sampled hidden state.

In our case, we take \textsc{Model 2} and apply hard attention at the sentence level, but keep the word level attention per sentence as is. That is, we sample from the attention weights $\alpha_1^s, \ldots, \alpha_m^s$ to obtain a one-hot encoding for the sentence attention, and apply the same multiplication with this one-hot vector on the word-level attention weights $\alpha_{i,1}^w, \ldots, \widetilde{\alpha}_{i,n_i}^w$ for all $i = 1, \ldots, m$. We call this \emph{Model 3}.


Because the hard attention model loses the property of being end-to-end differentiable, we turn to policy gradient methods from the reinforcement learning literature. %%%


\subsection{Practical considerations}

Recall the policy gradient update from section ??? at time $t$ of the decoder:
\begin{equation}
w \gets w + R_t \frac{\partial \log p(\balpha_t | y_1, \ldots, y_{t-1})}{\partial w}
\end{equation}
Since samples from $\balpha_t$ at time $t$ of the RNN decoder can also affect future rewards, we use a discount factor of $\gamma = 0.5$, so that the reward is $R_t = \sum_{s = t}^n \gamma^{n-s}r_s$ at time $t$, where $r_t = \log p(y_t | y_1, \ldots, y_{t-1})$ is the single step reward.

While this gradient is theoretically unbiased, because the gradient is sampled through a stochastic process, it tends to have high variance in practice. Thus, we implement a few variance reduction techniques to stabilize training.

\paragraph{Baseline reward}
One of the simplest ways to reduce the variance of the gradient estimator is to introduce a baseline reward $b$ which we subtract from our reward. Including a baseline is proven to reduce the variance of the estimator \citep{mnih2014belief}. \todo{should we prove this?}

%We also normalize the rewards to a common scale by dividing by the reward variance in a given minibatch.


Our policy gradient update then becomes
\begin{equation}
w \gets w + (R_t - b) \frac{\partial \log p(\balpha_t | y_1, \ldots, y_{t-1})}{\partial w}
\end{equation}

There are a few methods for producing the baseline; as in \cite{xu2015captioning}, we can keep an exponentially moving average of the reward
\begin{equation}
b \gets b + \beta (R_t - b)
\end{equation}
where $R_t$ is the average minibatch reward and $\beta$ is a learning rate (set to 0.9).

%Similarly, we keep a moving average of the variance for normalization:
%$$\sigma^2_j = (1 - \beta) \sigma^2_{j-1} + \beta v_j$$
%where $v_t$ is the variance of the minibatch rewards for batch $j$. Since we have rewards at each time step of the decoder LSTM, we keep a separate moving average for the baseline for each time step, but we keep a single moving variance for all time steps.

While several papers suggest using a learned baseline from the RNN state \citep{ranzato2015} in RL tasks, we have not found this to be more effective.

We also include an entropy-increasing term to encourage exploration during the training process, which is important to speed up convergence of learning. Our policy gradient equation then becomes
\begin{equation}
w \gets w + (R_t - b) \frac{\partial \log p(\balpha_t | y_1, \ldots, y_{t-1})}{\partial w}
 + \lambda_{ent} \frac{\partial H(\balpha_t)}{\partial \balpha_t}
\end{equation}
where $H(\balpha_t) = -\sum_{i=1}^n \alpha_t \log \alpha_t$ is the entropy function and $\lambda_{ent}$ is a hyperparameter.


%\subsection{Multiple Samples}
%
%From our initial experiments with Model 3, we found that taking a single sample was not very effective. However, we discovered that sampling multiple times from the distribution $\alpha$ significantly improves performance.
%
%We sample based on the multinomial distribution $\mathrm{Mult}(k, \{\alpha_i\}_{i=1}^n)$ to produce the sentence-level attention vector $\alpha$ of length $n$, with $\alpha_i = x_i / k$, where $x_i$ is the number of times index $i$ was sampled. $k$ is a hyperparameter which can be tuned, and we found that $k=5$ works well in our experiments.
%
%The intuition here is for the hard attention model to more closely approximate the soft attention model, as it can select more sentences to produce the context.

%\subsection{Curriculum}
%
%Since training using policy gradients tends to be noisy and slow to converge, we experimented with a curriculum that starts training with soft attention and in epoch $t$, trains a minibatch using hard attention with probability $p_t = 1 - 1/\sqrt{t}$ \citep{bengio2016hardntm}.
%
%While we found this to be helpful for single sample hard attention, it was not necessary for effective training with multisampled hard attention. We prefer to train solely with hard attention when possible, as we are able to save computation at training time.

%\section{Sparsemax}
%
%% describe


\chapter{Experiments}

\section{Data}
% if we do other experiments, include more here.

\subsection{CNN/Dailymail}

Experiments were performed on the CNN/Dailymail dataset from \cite{hermann2015read}. While the dataset was created for a question-answering task, the dataset format is suited for summary. Each data point is a news document accompanied by up to 4 ``highlights'', and we take the first of these as our target summary. Train, validation, and test splits are provided along with document tokenization and sentence splitting. We do additional preprocessing by replacing all numbers with \# and appending end of sentence tokens to each sentence. We limit our vocabulary size to 50000 most frequent words, replacing the rest with \texttt{<unk>} tokens. We dropped the documents which had an empty source (which came from photo articles).

 Table~\ref{data_stats} lists statistics for the CNN/Dailymail dataset.

% dataset statistics
\begin{table}[h]
\centering
\begin{tabular}{lcc}
\toprule
Dataset  & CNN & Dailymail \\
\midrule
Train size & 90267 & 196962 \\
Valid size & 1221 & 12149 \\
Average words per doc & 794 & 832\\
Average sents per doc & 21 & 29\\
Average words per sent & 36 & 27\\
Average words per summary & 13 & 15 \\
\bottomrule
\end{tabular}
\caption{Statistics for CNN/Dailymail data.}
\label{data_stats}
\end{table}

% examples of data needed


% is it even worth mentioning this?
%\cite{Chen2016} argue that the CNN/Dailymail is not the best dataset for 

\section{Synthetic Pretraining} % are we still doing this???

We found that unsupervised pretraining on the given dataset is beneficial to learning. For each document, we randomly sample 2 sentences and concatenate to form the target sentence in a new synthetic dataset. We can sample multiple times to have multiple targets for a given source document, and we found that 5 samples was most beneficial to learning (performance drops with significantly more samples). We then train on the synthetic dataset for 5 epochs and initialize future training with the learned weights.

\section{Implementation Details}

A few implementation details were necessary to make minibatch training possible. First, instead of taking attention over each individual sentence, we arrange the first 400 words of the document into a 10 by 40 image, and take each row to be a sentence. Second, we pad short documents to the maximum length with a special padding word, and allow the model to attend to it. However, we zero out word embeddings for the padding states and also zero out their corresponding LSTM states. We found in practice that very little of the attention ended up on the padding words.

Ideally, we would prefer to not truncate documents, especially since later context can be important for summarizing the document. Due to memory issues, this is a problem we still have to resolve.

\section{Models}

\subsection{Baselines}

For a baseline, we take the first 15 words of the document (chosen as the average length of a sentence in the training dataset). We call this \textsc{First}. 

Others...
% Need way more here...
% ideas: berkeley system (if it works), look for others - sentence fusion?


\subsection{Our models}

We ran experiments with Models 0 to 3 as described above. Model 0 serves as the baseline.

\begin{itemize}
\item Model 0: Soft attention.
\item Model 1: Hierarchical LSTM, soft attention over all.
\item Model 2: Hierarchical LSTM, soft hierarchical attention.
\item Model 3: Hierarchical LSTM, hard attention over sentences.
\item Model 3+multisampling: We include multisampling with $k=5$.
\end{itemize}

%% Help so bad!!!


\section{Training}

%% fix batch size and other details, and also note we train until convergence (which is slightly longer for hard attn)
We train with minibatch stochastic gradient descent (SGD) with batch size 32 for 13 epochs, renormalizing gradients to be below norm 5. We initialize the learning rate to 1, and begin decaying it by 0.5 each epoch after the validation perplexity stops decreasing. 

We use 2 layer LSTMs with 500 hidden units, and we initialize word embeddings with 300-dimensional word2vec embeddings \citep{mikolov2013word2vec}. For convolutional layers, we use a kernel width of 6 and 600 filters.

%Rewards for multisampling are scaled by a factor of $0.04$ (in addition to scaling by the moving variance).

In the next chapter we show results.

\chapter{Results}

\section{Evaluation} % other experiments? find other metrics
% also, where should we talk about the fact that ROUGE sucks?

We report metrics for best validation perplexity and ROUGE scores \citep{lin2004rouge}. We use the ROUGE balanced F-scores with ROUGE-1 (unigrams), ROUGE-2 (bigrams), and ROUGE-L (longest common substring). We chose F-scores since recall is biased towards longer sentences.

To generate summaries for evaluation, we run beam search with a beam size of 5.



\begin{table}[h]
\centering
\begin{tabular}{llcccc}
 \toprule
 Model &  & PPL & ROUGE-1 & ROUGE-2 & ROUGE-L \\
 \midrule
\textsc{First} & & - & 23.1 & 9.8 & 20.5 \\
\textsc{berkeley} \\
\textsc{Model 0} & &  \\
 \textsc{Model 1} & & \\
 \textsc{Model 2} & & 16.2 & 24.5 & 12.0 & 22.9 \\
\textsc{Model 2+SynthPre} & & 16.0 & 23.7 & 11.5 & 22.0 \\
 \textsc{Model 3} & &   \\
 \textsc{Model 3+sample5} & & 17 & 23.9 & 11.3 & 22.4\\
\textsc{Model 3+sample5+SynthPre} & & 14.6 & 24.1 & 11.7 & 22.5\\
 \bottomrule
\end{tabular}
\caption{Summarization results for CNN/Dailymail.}
\label{table:summary}
\end{table}

\section{Analysis}

See Table~\ref{table:summary} for summary results.

%% attention figures needed here

We notice that Model 2 has the best performance, while multisampling is comparable. We hypothesize that by sampling multiple times, the model learns to approximate the soft attention distribution 

\chapter{Discussion}


\chapter{Conclusion}


\bibliographystyle{plainnat}
\bibliography{../papers/Mendeley/library}


\begin{appendices}

\chapter{Attention Visualizations}

hi

\end{appendices}

\end{document}
