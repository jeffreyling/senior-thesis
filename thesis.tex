\documentclass[11pt]{report}
\usepackage{common}
\usepackage[round]{natbib}
\usepackage[strings]{underscore}
\usepackage{url}
\usepackage{todonotes}
\usepackage[toc]{appendix}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}


\title{Document Summarization (draft)}
\author{Jeffrey Ling}
%\date{}
\begin{document}
\maketitle{}

\begin{abstract} % needs work
Standard sequence-to-sequence (seq2seq) attention models have seen great success in NLP, but do not scale well to tasks with long sequences. We propose a novel coarse-to-fine attention method to reduce the number of computations necessary in standard attention. By organizing the source sequence into a 2-dimensional image, we hierarchically apply attention, using a coarse mechanism for the first layer to select a row sequence, and a finer mechanism for the second soft attention layer. While the computation for training standard seq2seq models scales linearly with source sequence length, our method is invariant to length and thus can scale arbitrarily.

We evaluate our model on the CNN/Dailymail document summarization task.
\end{abstract}

\tableofcontents{}




\chapter{Introduction}

\section{Natural Language Processing}

Natural language processing is a field with a variety of interesting structured prediction problems. The essential goal of NLP is to build a model of language so that computers can automatically process substantial quantities of text --- a highly relevant problem in today's information age.

While humans have no trouble understanding and using language, even the simplest language tasks can be impossible for computers. Researchers developed the field of linguistics as they tried to build a formal model of how language works, and as the number of rules grew, computational methods for processing text soon became relevant. Today, NLP has transformed into its own subfield --- some classical linguistic problems still remain relevant, such as part-of-speech tagging, parsing, and semantic modeling, but more computational approaches are also used to tackle problems like language modeling and machine translation. %% 

It is informative to consider the history of machine translation. The first methods were rule-based linguistics systems, and the company SYSTRAN provided one of the first widely available translation tools. %% citation

Later, statistical methods for NLP became increasingly popular. By considering language generation as a probabilistic process, one can collect statistics over large corpuses of data to automatically deduce the parameters of the model. In machine translation, this idea was used to great effect in \citet{Brown1993}, whose count-based methods form the core of state-of-the-art systems like Google Translate.

Recently, researchers found that deep learning works effectively for many NLP tasks. For the first time, neural networks were able learning structure and features from language almost completely from scratch \citep{Collobert2011}. % \cite{Bengio2003} language model
The success of neural methods has been adopted by Google Translate to build even better systems \citep{GoogleTranslate2016}. Research in applying deep learning to NLP is ongoing.

\section{Deep Learning}

The history of neural networks dates back to the perceptron \citep{Rosenblatt1958}, a simple model that assumes data can be linearly separated. Due to this strict requirement, the machine learning community dismissed the idea as impractical, and research was sidelined for most of the 20th century.

Some other work continued: \citet{Rumelhart1986} showed that the backpropagation algorithm could be used to efficiently train general neural networks, but this was before the era of GPUs and modern computing, and so the algorithm could not be practically used.

Recently, neural networks have made a resurgence. In the ImageNet image classification competition in 2012, \citet{Krizhevsky2012} won using \emph{deep convolutional neural networks} \citep{LeCun1995}, beating the competition by a significant margin. This led to a renewed wave of research, especially due to the advancement of modern computing power and GPUs, which can train networks at 10 or 20 times the speed of standard CPUs. Today, deep models are used to successfully play Go \citep{Silver2016}, play Atari games from pixels \citep{deepatari2015}, and ...... %%%??

%%% break

While neural networks are often treated as black box classifiers, \citet{Zeiler2014} show that the intermediate layers of deep convolutional networks contain abstracted qualities of the input, such as patterns, textures, and objects of the input. This suggests that neural networks are discovering features of the input and building generalized \emph{representations} of their inputs.

\todo{figure}

The idea of learning representations of the input is highly general, and so it comes as no surprise that deep networks soon found applications in NLP. \citet{mikolov2013word2vec} show that by training a neural network on a Google News text corpus, the network learned to map words in the English language to a vector of real numbers known as \emph{word embeddings}. These word embeddings are actually able to capture semantic properties of the words --- for example, taking the vectors for \emph{king}, \emph{man}, and \emph{woman}, we find that $v_{king} - v_{man} + v_{woman} \approx v_{queen}$, preserving the analogy that we usually make with text.

% diagram of vectors

Since the onset of deep learning, deep models have found their way into nearly every corner of NLP. Much of their success relies on the ubiquity of the \emph{long short-term memory} (LSTM) recurrent neural network \citep{hochreiter1997long}, a model used to both process and generate sequences of text. State-of-the-art systems for many problems, including machine translation, now use deep learning as their core algorithm. While there is too much to cover here, \citet{goldberg2015primer} provides a concise summary of the models that have had the greatest impact on NLP. % weak


\section{Motivation}

% should this go under deep learning?
% this could be a hard section to write

Why deep models? There are two general approaches for solving NLP problems today: one is to use as much as linguistic theory as possible to reduce the problem, and another is to apply black box models such as deep neural networks.

On one hand, systems with handcrafted features can perform extremely well if domain specific knowledge can be built in. \todo{Examples?}
On the other hand, a purely data-driven system can forgo most of these details, using highly general models for the task at hand.

There are a few reasons why deep networks are desirable for NLP. First, they work remarkably well without any feature engineering, and are state-of-the-art in many existing tasks.
Second, they are not mutually exclusive with standard feature extraction methods.
Third, we find that trained models can discover latent structure in language automatically, which is an interesting phenomenon that may reveal more about how language is used. As an example, sequence-to-sequence models with attention \citep{bahdanau2014neural} learn the concept of a word alignment in translation without any supervision.

It is not yet clear, however, to what extent neural methods can replace models with hard-coded linguistic assumptions. We set out to understand this question by analyzing and extending a particular deep neural model for the task of document summarization.


\section{Our problem}

Text summarization is an important problem for compressing large bodies of natural text into a more easily digestable form. Document summarization is one of the most challenging formulations of this problem, where given a document with several sentences of text, the goal is to produce a coherent summary that captures most or all of its salient points.

To accomplish this, we use the most recent advances in deep learning to automatically produce summaries by training on a large dataset of given examples.

We can frame summarization as a supervised sequence-to-sequence task: given a news article (the source sequence of words), we desire to produce a summary (the target sequence). Existing methods in deep learning have been developed and proven to be highly effective for this kind of task, especially the sequence-to-sequence (seq2seq) model applied to machine translation \citep{sutskever2014sequence, bahdanau2014neural}. \citet{rush2015neural} uses the seq2seq model to summarize sentences into shorter headlines, but we consider the more general problem of arbitrarily long documents.

Existing seq2seq methods are limited by the length of source and target sequences. For a problem such as document summarization, the source sequence of length $N$ requires $O(N)$ seq2seq model computations. However, it makes sense intuitively that not every word of the document will be necessary for generating a summary, and so we would like to reduce the amount of necessary computations over the source document.

Therefore, in order to scale seq2seq methods for this problem, we aim to prune down the length of the source sequence in an intelligent way. In the problem of image captioning, \citet{xu2015captioning} show that when generating words of the caption, the model places attention on the relevant part of the image.
Inspired by this idea, we introduce the coarse-to-fine attention mechanism that sparsely selects subsets of the source document for processing.


We provide an outline for the rest of this thesis.
In section ? we cover related work in summarization and deep learning.
In section ? we give the necessary background for our models.
In section ? we describe our models in detail.
In section ? we show results and discussion.

%%%%%%%%%%

\chapter{Related Work}

\section{Automatic Summarization}

\citet{Nenkova2011} give a detailed overview of the problem of summarization. In particular, they provide a taxonomy of methods that researchers have used to tackle the task:

\paragraph{Extractive vs. abstractive} Extractive summaries extract certain sentences or phrases from the document, while abstractive summaries take a more holistic view and can in practice be anything (similar to how humans produce summaries). 

\paragraph{Single- vs. multi-document} Summarization was originally posed as the problem of producing a summary for a single document. However, with the onset of the Internet and search engines, there are often multiple documents on the same topic, and so a summarization system should be able to use all of them to produce a summary.

%\paragraph{Indicative (style) vs. informative (facts)} Indicative summaries provide a sense for what a certain document is about without necessarily giving all of its details, while informative summaries are meant to replace the original document in terms of content.

\paragraph{Keyword vs. headline} Keyword summaries are allowed to simply be a bag-of-words of important keywords from the document, while headline summaries must form a coherent sentence.

\paragraph{Generic vs. query focused} Generic summaries have no assumptions on the reader and are meant to be generally informative, while query focused summaries take into consideration a query and only return relevant information. The contrast between these two methods highlights an important question in summarization: to what end are we summarizing documents? If we can answer this question more precisely, we will be better able to build systems to accomplish our desired tasks.

\vspace{0.5cm}

Before elaborating on specific methods, it is important to highlight these differences to understand how people thought about the problem. For example, extractive methods are by far more popular due to the simplicity of building an extraction algorithm, while abstractive methods have been difficult as we do not have a fully working model of language generation.

Based on this taxonomy, the deep model we set out to build would be classified as: abstractive, single-document, headline, and generic. While there is value to exploring other branches of this taxonomy, this combination of categories is the simplest for a deep model to handle.

In the rest of this thesis, we will limit the scope of the summarization problem to the single-document,  headline, and generic summarization problem. In the next section, we give a brief overview of some of the relevant methods used.


\section{Methods}



\subsection{Classical}

One of the first considerations of automatically producing summaries was \citet{luhn1958automatic}, which pioneered work in the field. At the time, computers still ran on punch cards, so automating the summarization process was no easy feat.

\citet{luhn1958automatic} proposed a sentence-ranking method to produce summaries. The algorithm gives each sentence a score based on the occurrence of frequently appearing words. This is one of the first examples of an \emph{extractive} summarization method.

Since then, a variety of approaches have been used to solve summarization. We highlight some notable work in both the extractive and abstractive framework.

\paragraph{Extractive} The most popular methods for document summarization have generally been extractive due to their simplicity. One natural procedure for an extractive summarization is to score sentences based on some relevance metric and return the highest scoring sentences.

Some examples are \cite{carbonell1998MMR}, which uses a simple information metric for ranking sentences, and \cite{svore2007}, which uses a basic neural network for the same purpose.

\citet{Shen2004} models sentence extraction as a sequential decision problem, using a linear-chain conditional random field to find the best subset of sentences.

\todo{details}


\paragraph{Abstractive} While extraction has proven to be successful, the method is inherently limited in its ability to summarize. The more challenging method, and also the closest to what humans do, is \emph{abstractive} summarization. Instead of strictly requiring that all words of the summary come from the source document, any coherent text is allowed.

Two methods used to produce abstractive summaries are sentence compression and sentence fusion. Compression removes less useful information from sentences, while fusion is harder and combines information from sentences. % cite Barzilay, MCkeown

Compression: \citet{knight2002summarization} employs a noisy channel model, similar to machine translation, to deduce the ``most probable'' compression, while \citet{clarke2008global} uses an integer linear program. \citet{cohn2008sentence} extend the tree-based methods to allow for insertions and substitutions during compression, whereas prior methods were purely deletion based. \cite{zajic2004topiary} successfully use a sentence compression algorithm along with an unsupervised topic model on the DUC 2004 task.

Fusion: align parse trees and combine phrases that are similar \todo{finish}


\subsection{Deep Learning}

With the onset of deep learning, learning an end-to-end abstractive model for summarization has become more feasible. \citet{rush2015neural} propose a data-driven, completely abstractive model for summarizing short sentences by training a sequence-to-sequence model with attention. More recent work in deep learning has been done for both extractive \citep{} %% citation needed
and abstractive \citep{nallapati2016seq2seq} methods that scale the models to full documents, demonstrating the feasibility of end-to-end models.

These new models require a large amount of supervised training data, which previously was not available. However, thanks to the Internet, the web can easily be scraped to produce large-scale annotated datasets to train our deep models.


\section{Datasets}

To standardize the summarization task, NIST released data for DUC (Document Understanding Conferences) between 2001-2007 \citep{over2007duc}. The DUC tasks involved producing summaries for both single- and multiple-document sets of news articles. DUC 2001 and 2002 ask for general summaries of these articles documents and summaries, while DUC 2003-2006 also evaluate summaries based on their usefulness for certain question-answer tasks.


DUC overall was not particularly impactful. For many of the news summary tasks, it was found that taking the first sentence of each article was a strong baseline that more sophisticated methods found hard to beat. However, DUC inspired a lot of thinking on the best way to evaluate summaries (see below).

Recent datasets are much larger thanks to the power of scraping the Internet. \citet{Hermann2015} released the CNN/Dailymail for question answering, which can also be adapted for summarization. Each document is a news article accompanied by three or four ``highlight'' bullet points, which we can treat as the summaries. We will use the CNN/Dailymail dataset to train our deep models.

In the context of these new datasets, the summarization task has not yet been fully standardized. Research in the area is still largely preliminary, with only a few papers reporting results \citep{nallapati2016seq2seq}. While CNN/Dailymail may not be the most suitable dataset for the task due to its noisiness \citep{Chen2016}, a better alternative is yet to exist.

\section{Evaluation}

Evaluating a good summary is inherently ambiguous, and probably one of the hardest parts of the problem.

For extractive summaries, people have proposed simple metrics such as precision and recall on selected sentences. These naturally do not work too well since 1) not all sentences are equally informative, and 2) not all parts of a sentence are relevant.

The DUC conferences really pushed forward understanding on evaluation. While a single most effective metric for summarization may not exist, DUC established several important criteria, including grammaticality, non-redundancy, and content coverage.

In response, people came up with recall on elementary discourse units (EDUs), based on clauses within a summary that ought to be captured. ROUGE \cite{lin2004rouge} is cheap and fast. Pyramid method is a complicated human evaluation method based on summary content units (SCUs). 

None of these methods directly address the grammaticality of the output. Aside from using human evaluation, meaningful metrics for summaries is still very much an open question \citep{toutanova2016summarymetrics}. In our work, we settle for ROUGE due to its cheapness and ease of use in evaluating our models.


\section{In the wild}

Summarization is an important real-world problem due to the explosion of available data. Thus, there are many practical methods that have been developed and deployed in real-world settings. One example is on Reddit\footnote{\url{reddit.com}}: in order to summarize long forum discussions, Reddit uses technology from \textsc{smmry}\footnote{\url{smmry.com}}.

Smmry's algorithm is a simple extractive summarization method. It counts word occurrences, splits discussions by sentence, and ranks the sentences based on the sum of their word scores (perhaps tf-idf?). This algorithm bears extraordinary similarity to \citet{luhn1958automatic} --- although a variety of methods have been invented since then, the simplest approaches turn out to be the most practical.


%%%%%%%
\chapter{Background}

In this chapter, we set up the relevant background ideas for our models.

\section{Sequence-to-Sequence Attention Models}

The sequence-to-sequence architecture \citep{sutskever2014sequence}, also known as the encoder-decoder architecture, forms the backbone of many successful models in NLP. A popular variant of sequence-to-sequence models are \emph{attention} models \citep{bahdanau2014neural}. The key idea is to keep an encoded representation of all parts of the input, attending to the relevant part each time we produce an output from the decoder.
These models have been used to great effect in a variety of NLP tasks, including machine translation \citep{sutskever2014sequence, bahdanau2014neural}, question answering \citep{Hermann2015}, dialogue \citep{li2016persona}, caption generation \citep{xu2015captioning}, and in particular summarization \citep{rush2015neural}. In particular, these works show that learning an accurate attention function is important for good performance. %%???

\citet{xu2015captioning} show how attention models can be used to ``summarize'' an image and produce a caption. 
By analyzing where in the image their models attend to when generating each word of the caption, they qualitatively find that the model is essentially describing that region of the image. 
Figure~\ref{fig:caption} shows some examples.

\begin{figure}
\label{fig:caption}
\end{figure}

We can leverage the same idea for text summarization, assuming we have a suitable representation of our input document. The simplest method for doing so would be to run an LSTM over the document.

However, the attention step becomes computationally difficult --- for each word we generate, we need to compare it to every word of the document in order to determine which part to attend to. Therefore, we propose a hierarchical method of attending to the document by first attending to sentences, then to the words within sentences. We call this method \emph{coarse-to-fine attention}
\footnote{The term coarse-to-fine attention has previously been introduced in the literature \citep{mei2016}. However, their idea is different: they use coarse attention to reweight the fine attention computed over the entire input. Similar ideas have also been called hierarchical attention \citep{nallapati2016seq2seq}.}.



To be able to attend to both sentences and words in a hierarchical manner, we need to construct encodings of the document at both levels. Thus, we run a low-level LSTM encoder on the words of each sentence for a fine-grained representation of the text, and a simpler encoder model (e.g. bag of words) for coarse-grained sentence representations.
\citet{Sukhbaatar2015} demonstrate how coarse representations can be useful by using memory networks to access information for simple question-answering tasks.
\citet{li2015autoencoder} use the idea of coarse and fine encodings to develop a hierarchical autoencoder for representing paragraphs of text.
%by using low-level LSTMs on words to build sentence representations, then a high-level LSTM on the sentences for the final paragraph representation.

Therefore, if we can make our model first use coarse attention to choose sentences, then use fine attention to choose words only from that sentence, then we avoid the computational cost of searching over the entire document. This idea runs into a very serious problem, however: by posing the attention as a discrete selection process, the neural network loses the key property of being differentiable, and so standard backpropagation no longer applies.


\citet{xu2015captioning} suggest ``hard'' attention as one possible solution to the discrete selection problem. While standard ``soft'' attention actually averages the representations of where the model attends to, for hard attention we make a hard decision and choose only one location. Such models can be trained using reinforcement learning. Before we elaborate on this method, we survey some other methods invented to overcome this discrete attention problem.

\section{Conditional Computation}


%The problem of full document summarization, however, is still very open. In order to make the models work, \cite{nallapati2016seq2seq} use a variety of tricks: limiting the decoder vocabulary to the document, the use of pointer attention for \texttt{<unk>} tokens, etc. Our goal in this paper is not necessarily to optimize performance, but to understand how to scale up existing seq2seq methods in an efficient way, and so we attempt to eliminate the use of these ad-hoc tricks whenever possible. We aim to find the best implementation of ``hard'' attention, in which a discrete subset of the source document is selected at any given time, to satisfy the scalability condition.

\todo{make this section more coherent}

Many techniques have been proposed in the literature to handle the problem of large inputs to deep neural networks.

The term ``conditional computation'' was coined by \citet{BengioLC13}, where the idea is to compute a subset of units for a given network per input. This would have the advantage of being much more efficient, especially for networks that handle extremely large inputs as is common in vision and NLP.

Several ideas have been proposed to conditionally compute on large inputs. 
\citet{rae2016sparsememory} use an approximate nearest neighbors approach for their ``sparse access memory'' model to train a large-scale neural Turing machine. \citet{Shazeer2017} introduces a mixture-of-experts model that selectively chooses a subset of ``expert'' networks at any given time during training. In the spirit of conditional computation, they only train $K$ experts at a time using a sparse gating function.


\subsection{Discrete units}

One particular class of methods to perform conditional computation relies on the use of discrete units within our deep network. Discrete units can be produced using stochastic gates to select certain parts of the network for computation.
Unfortunately, discrete variables cannot be backpropagated through as they are not differentiable. Several techniques have been proposed to get around this problem.

\citet{BengioLC13} propose the \emph{straight-through} estimator for binary stochastic gates. We simply sample from the stochastic gates in the forward step, and backpropagate the gradient as if we had not sampled. While this works for simple binary gates, it is theoretically unjustified.

In general, we usually want to sample from multinomial distributions given by the output of a softmax function. The function $\operatorname{softmax} : \reals^m \to [0,1]^m$ produces a normalized probability distribution, where for $\boldz \in \reals^m$,
\begin{equation}
\operatorname{softmax}(\boldz)_i = \frac{\exp(z_i)}{\sum_j \exp(z_j)}
\end{equation}

\citet{martins2016sparsemax} propose an alternative to softmax called the \emph{sparsemax} function. For a given $\boldz$, sparsemax projects the point to the probability simplex. It turns out that this function has a useful gradient while having a sparse output. The downside is that we are not guaranteed to have a one-hot vector as we get from sampling the multinomial distribution.

\citet{Maddison2017} apply a smoothed version of the Gumbel max trick in order to approximate the sampling process. The Gumbel max trick is an alternative method for sampling multinomial random variables: by drawing i.i.d. uniform variables $U_i \sim \mathrm{Unif}(0,1)$, taking the argmax of $z_i - \log(-\log(U_i))$ gives us a sample with the correct probability. While this process is still discrete and has no gradient, \citet{Maddison2017} use a softmax instead of a max to obtain a smooth approximation that can be differentiated.


Reinforcement learning has also been proposed as an approach to handling discrete units in a network. In this thesis, we focus on the hard attention method of \citet{xu2015captioning}. In the next section, we explain reinforcement learning and how it can be used to train the hard attention model.

\section{Reinforcement Learning}

Standard backpropagation training of neural networks assumes that the output is a deterministic and differentiable function of the input. Reinforcement learning, however, is a more general framework that makes no such assumptions.

The traditional setup of reinforcement learning (RL) assumes some agent is navigating an environment and earning rewards.
We assume a state space $\mcS$, an action space $\mcA$, a reward function of state and action $R : \mcS \times \mcA \to \reals$, and a transition distribution $p(s_{t+1} | s_t, a_t)$ (so that our environment is Markovian).

We suppose that at time $t$, the agent is in state $s_t \in \mcS$, makes an action $a_t \in \mcA$, earns a reward $r_t = R(s_t, a_t)$, and transitions probabilistically to the next state $s_{t+1}$.
Assuming the reward function is unknown, the agent wants to maximize total expected reward
$$\mathbb{E}_{s_t, r_t} [\sum_{t=0}^\infty \gamma^t r_t]$$
by finding an optimal action policy. Here, $\gamma$ is a time discount factor.

% diagram

\subsection{Methods for training RL agents}

A variety of methods have been invented for solving the RL problem, i.e. finding the optimal policy $\pi(s)$ for a given state $s$.

When the transition distribution $p(s_{t+1} | s_t, a_t)$ and reward function $R(s,a)$ is known, our problem is also known as a Markov decision process (MDP). We can compute an optimal value function $Q(s, a)$ that represents the estimated reward from taking action $a$ in state $s$:
\begin{align}
Q(s, a) & = R(s, a) + \gamma \sum_{s' \in \mcS} p(s' | s, a) V(s') \\
V(s) &= \max_{a \in \mcA} Q(s,a)
\end{align}
for every $s \in \mcS, a \in \mcA$. These equations are known as the Bellman equations, and finding an optimal policy then amounts to solving this system for $\pi$. This can be done by treating the system as a fixed point problem and using iterative methods. % citation

When we don't know the transition distribution, we can estimate it by standard maximum-likelihood methods and exploration of the state space. The same iteration techniques then apply.

When we don't know the reward function \emph{and} we don't know the transition distribution, things become trickier. While the environment still gives us rewards for actions, finding the optimal policy requires knowing which states lead to those rewards.
Since we don't know ahead of time which states are best, we are forced to extensively explore the state space to find what actions lead to the best rewards.

There are two methods for approaching the general RL problem. First, model-based approaches attempt to estimate the transition distribution $p(s_{t+1} | s_t, a_t)$ and apply the Bellman equations to find the optimal policy. Second, model-free approaches forgo the transitions and simply learn what action is best in a given state.

\subsection{Model-free approaches}

We consider the model-free approach in more detail.

\paragraph{Q-learning} One technique is to model the Q-function $Q(s,a)$ that estimates the total reward of action $a$ in state $s$. To learn the Q-function, we predefine some policy based on our current estimates of the Q-function, and we make learning updates as
\begin{equation}
Q(s,a) \gets Q(s,a) + \eta \left(r + \gamma \max_{a' \in \mcA} Q(s', a') - Q(s,a) \right)
\end{equation}
upon taking action $a$ in state $s$, where $\eta$ is a learning rate, $r$ is the reward received, and $s'$ is the state we transition into. This is known as the \emph{Q-learning} update rule. An alternative update rule takes into account the action $a'$ we would take next in state $s'$:
\begin{equation}
Q(s,a) \gets Q(s,a) + \eta  \left(r + \gamma Q(s', a') - Q(s,a) \right)
\end{equation}
This is known as the SARSA update rule.

In both cases, if we have a large state space, we may not want to record $Q(s,a)$ for every pair of $s,a$. We can instead parametrize $Q(s,a)$ with weights $w$, and maximizing reward using backpropagation gives us the update rule
\begin{equation}
\label{eq:reinforce}
w \gets w + \eta \left(r + \gamma \max_{a' \in \mcA} Q(s', a') - Q(s,a) \right) \frac{\partial Q(s,a)}{\partial w}
\end{equation}

\paragraph{Policy gradient} An alternative to modeling the Q-function is to learn a policy $\pi : \mcS \to \mcA$ directly. We parametrize $\pi$ with weights $w$, and assume that at training time, we have a probability distribution $p(a|s ; w)$ over the actions that we sample from. Then assuming we get reward $r$, our update is
\begin{equation}
w \gets w + \eta \mathbb{E}_a \left[ r \cdot \frac{\partial \log p(a | s ; w)}{\partial w}  \right]
\end{equation}

This is known as the policy gradient update or REINFORCE algorithm \citep{williams1992reinforce}. We give the derivation in the next section.

If we have a full trajectory of states $s_t$ and rewards $r_t$, then we can assume our action $a_t$ at time $t$ led to future discounted reward $R_t = \sum_{k=t}^\infty \gamma^{k-t} r_k$. We then have the update rule
\begin{equation}
\label{eq:reinforce1}
w^{(t+1)} \gets w^{(t)} + \eta \mathbb{E}_{a_t} \left[ R_t \cdot \frac{\partial \log p(a_t | s_t ; w)}{\partial w}  \right]
\end{equation}

%\paragraph{TD-learning} An alternative to modeling the Q-function is to model the value function $V(s) = \max_{a \in \mcA Q(s,a)$, which can be a simpler learning problem. 
%
%Again, suppose we parametrize $V(s)$ with weights $w$, and suppose we have an observed sequence of states $s_t$ and rewards $r_t$. Instead of making updates to $V(s)$ independently for each time $t$, we can take into account our entire trajectory, giving the update rule at time $t$
%\begin{equation}
%w \gets w + \eta \left(r_t + \gamma V(s_{t+1}) - V(s_t) \right) \sum_{k=1}^t \lambda^{t-k} \frac{\partial V(s_k)}{\partial w}
%\end{equation}
%where $\lambda$ is a discount factor for past gradients.
%
%This update is known as the $TD(\lambda)$ update, and is a special case of temporal difference learning (or TD-learning). Taking into account the entire trajectory allows us to obtain more information about which past actions and states led to the best rewards. TD-learning was used to train a world-class backgammon bot \citep{Tesauro1994}.

%This raises two problems. First is the \textbf{credit assignment problem}: we don't know which actions we chose in the past led us to the best rewards. For example, if we are one move away from winning a game and earning a big reward, we don't assign the credit of the reward to that one move, but rather to some good moves we made in the past that led us to the winning state.
%Second is the \textbf{exploration-exploitation problem}. On one hand, we need to explore the state space to determine which states have the best rewards, but on the other hand, we are missing the opportunity to exploit the known high-reward actions.

%occasionally choosing random actions to better explore the state space (also known as an $\epsilon$-greedy exploration policy).


\section{Deep reinforcement learning}

Reinforcement learning and deep learning have successfully been combined to play Go \citep{Silver2016}, 
control robots \citep{Levine2016}, and play Atari games from pixels \citep{deepatari2015}.

Aside from these classical applications of RL, the RL framework can be applied to train deep neural networks with stochastic units. We cast our learning problem with the neural network as an agent with a parametrized policy function $\pi : \mcS \to \mcA$, where the states $\mcS$ are the inputs and the actions $\mcA$ are the possible outputs of each stochastic unit. The rewards are then negative loss (e.g. from regression or classification).

More concretely, consider a sequential classification problem such as summarization, and consider the sequence-to-sequence model with attention. At each time step $t$ of the decoder, our state $s_t$ is given by the LSTM state and the encoded context, and our action $a_t$ is where in the encoder context to attend to. The reward $r_t$ is then the log-likelihood of predicting the gold word (according to some supervised dataset).

\todo{diagram}

To obtain gradients for backpropagation, we can apply policy gradient methods for the nondifferentiable stochastic units. Because our network has both differentiable and nondifferentiable weights on the pathways to the output, we can consider it as a \emph{stochastic computation graph} \citep{schulman2015backprop}.

Suppose that, given input $x$, our network has stochastic units drawn from some parametrized distribution $z \sim p(z | x; \phi)$. Then, a parametrized deterministic scalar function $f(z ; \theta)$ gives us the output, and we want to maximize the expectation $\mathbb{E}_{z \sim p(z | x; \phi)} [ f(z ; \theta) ]$.

To train the weights directly connected to the loss function (i.e. $\theta$) we backpropagate the standard gradient:
\begin{equation}
\frac{\partial \mcL}{\partial \theta} = \frac{\partial}{\partial \theta} \mathbb{E}_{z \sim p(z | x; \phi)} [ f(z ; \theta) ] =  \mathbb{E}_{z \sim p(z | x; \phi)} \left[ \frac{\partial f(z ; \theta)}{\partial \theta} \right]
\end{equation}
To train weights that are not directly connected, but precede an intermediate stochastic unit (i.e. $\phi$), we backpropagate the weighted policy gradient:
\begin{align*}
\frac{\partial \mcL}{\partial \phi} &= \frac{\partial}{\partial \phi} \sum_z p(z | x; \phi) f(z ; \theta) \\
& = \sum_z \frac{ \partial  p(z | x; \phi) } {\partial \phi} \cdot f(z ; \theta) \\
&= \sum_z p(z | x; \phi) \frac{1}{p(z|x;\phi)}\frac{ \partial  p(z | x; \phi) } {\partial \phi} f(z ; \theta) \\
&= \sum_z p(z | x; \phi) \frac{ \partial  \log p(z | x; \phi) } {\partial \phi} f(z ; \theta) \\
\frac{\partial \mcL}{\partial \phi} &= \mathbb{E}_{z \sim p(z | x; \phi)} \left[ \frac{ \partial  \log p(z | x; \phi) } {\partial \phi} f(z ; \theta) \right] \numberthis
\end{align*}
For a given $x$, we perform Monte Carlo sampling to obtain the gradients, e.g.
\begin{equation}
\frac{\partial \mcL}{\partial \phi} \approx \frac{1}{N} \sum_{i=1}^N \frac{ \partial  \log p(z_i | x; \phi) } {\partial \phi} f(z_i ; \theta)
\end{equation}
where $z_i \sim p(z | x; \phi)$ i.i.d. for $i = 1, \ldots, N$. In practice, $N$ is set to 1 for computational reasons.

Note that this generalizes the REINFORCE algorithm in equation~\ref{eq:reinforce}, where $f(z ; \theta)$ is the reward.


\subsection{Baseline reward}
In practice, the variance of the Monte Carlo gradient can be very high. One of the simplest ways to reduce the variance of the gradient estimator is to introduce a baseline reward $b$ which we subtract from our reward. This works because
\begin{equation}
\mathbb{E}_z \left[ (f(z; \theta) - b) \frac{\partial \log p(z | x; \phi)}{\partial \phi} \right] = \mathbb{E}_z \left[ f(z ; \theta) \frac{\partial \log p(z | x; \phi)}{\partial \phi} \right] - b \mathbb{E}_z \left[ \frac{\partial \log p(z | x; \phi)}{\partial \phi} \right]
\end{equation}
The second term vanishes as
\begin{align*}
\mathbb{E}_z \left[ \frac{\partial \log p(z | x; \phi)}{\partial \phi} \right] &= \sum_z p(z | x; \phi) \frac{\partial \log p(z | x; \phi)}{\partial \phi} =  \sum_z p(z | x; \phi) \frac{1}{p(z | x; \phi)} \frac{\partial  p(z | x; \phi)}{\partial \phi}  \\
&= \frac{\partial}{\partial \phi} \sum_z p(z | x; \phi) =  \frac{\partial}{\partial \phi} [1] = 0
\end{align*}
Thus, the gradient with baseline is unbiased.

Including a baseline is proven to reduce the variance of the estimator \citep{Weaver2001}. Intuitively, we can think of it as an estimate of the value function $V(s)$ for a state $s$. There are a few ways to produce a baseline for a given reward, which we describe in section ????.

Our policy gradient update from equation~\ref{eq:reinforce1} can therefore be summarized as
\begin{equation}
w^{(t+1)} \gets w^{(t)} + \eta \mathbb{E}_{a_t} \left[ (R_t - b) \frac{\partial \log p(a_t | s_t; w)}{\partial w} \right]
\end{equation}


\subsection{Deep RL in NLP}

In computer vision, deep reinforcement learning has been applied with some success 
\citep{mnih2014visualattention, ba2015visualattention, xu2015captioning}.

In NLP, RL has been attempted with varying degrees of success so far.
\citet{ranzato2015} apply RL to improve sequential estimation by e.g. training on BLEU rewards for machine translation.
\citet{Narasimhan2015} uses RL play text-based games.
\citet{li2016dialogueRL} uses RL to train an end-to-end dialogue system.
\citet{narasimhan2016} uses RL to improve information extraction on scarce data.
%\citet{Andreas2016} uses RL to compose 
\citet{Yogatama2017} uses RL along with an architecture called a tree LSTM that produces a tree-structured latent variable on a sentence using \textsc{Shift} and \textsc{Reduce} actions. They show that the tree LSTM can produce reasonable parse structures by solely using RL methods.


Inspired by these ideas, we handle the document summarization problem by (1) implementing conditional computation using coarse-to-fine attention at the sentence level, and (2) including reinforcement learning for a sparse coarse attention layer.

In the next chapter we describe our models in detail.



\chapter{Models}

\section{Sequence-to-sequence (seq2seq)}

We first describe the neural network architecture of the seq2seq models, also known as encoder-decoder models \citep{bahdanau2014neural}.

\subsection{Recurrent encoder and decoder}

In the seq2seq model, an \emph{encoder} recurrent neural network (RNN) reads the source sequence as input to produce the \emph{context}, and a \emph{decoder} RNN generates the output sequence using the context as input.  One popular RNN choice is the long-short term memory (LSTM) network \citep{hochreiter1997long}.

% more specifics here?
More formally, suppose we have a vocabulary $\mcV$. A given input sentence $w_1, \ldots, w_n \in \mathcal{V}$ is transformed into a sequence of vectors $\boldx_1, \ldots, \boldx_n \in \mathbb{R}^{d_{in}}$ through a word embedding matrix $\boldE \in \mathbb{R}^{|\mathcal{V}| \times d_{in}}$ as $\boldx_t = \boldE w_t$.

An RNN is given by a parametrizable function $f_{enc}$ and a hidden state $\boldh_t \in \mathbb{R}^{ d_{hid}}$ at each time step $t$ with $\boldh_t = f_{enc}(\boldx_t, \boldh_{t-1})$. For the LSTM, we keep an auxiliary state $\boldc_t$ along with $\boldh_t$, and we compute $f_{enc}$ as
\begin{align}
\boldf_t &= \sigma(\boldW^f \boldx_t + \boldU^f \boldh_t + b_f) \\
\boldi_t &= \sigma(\boldW^i \boldx_t + \boldU^i \boldh_t + b_i) \\
\boldo_t &= \sigma(\boldW^o \boldx_t + \boldU^o \boldh_t + b_o) \\
\boldc_t &= \boldf_t \odot \boldc_{t-1}  + \boldi_t \odot \tanh(\boldW^c \boldx_t + \boldU^c \boldh_{t-1} + b_c) \\
\boldh_t &= \boldo_t \odot \tanh(\boldc_t)
\end{align}
where $\boldW, \boldU, b$ are learned parameters, and $\odot$ is the elementwise product. 
Intuitively, $\boldc_t$ is the memory cell, $\boldf_t$ is the forget gate, $\boldi_t$ is the input gate, $\boldh_t$ is the output cell, and $\boldo_t$ is the output gate.

LSTMs can be stacked on top of one another by treating the outputs $\boldh_t$ of one LSTM as the inputs to another LSTM.

In stacked LSTMs, we will take the top sequence of hidden states $\boldh_1, \ldots, \boldh_n$ to form a single context vector.

%% diagram

The decoder is another RNN $f_{dec}$ that generates output words $y_t \in \mathcal{V}$. It keeps hidden state $\boldh_t^{dec} \in \mathbb{R}^{d_{hid}}$ as $\boldh_t^{dec} = f_{dec}(y_{t-1}, \boldh_{t-1}^{dec})$ similar to the encoder RNN.
A context vector is produced at each time step using an attention function $a$ that takes the encoded hidden states $[\boldh_1, \ldots, \boldh_n]$ and the current decoder hidden state $\boldh_t^{dec}$ and produces the context $\boldc_t \in \reals^{d_{ctx}}$:
\begin{equation}
\boldc_t = a([\boldh_1, \ldots, \boldh_n], \boldh_t^{dec})
\end{equation}
As in \citet{luong2015effective}, it is helpful to feed the context vector at time $t-1$ back into the decoder RNN at time $t$, i.e. $\boldh_t^{dec} = f_{dec}([y_{t-1}, \boldc_t], \boldh_{t-1}^{dec})$.

Finally, a linear projection produces a distribution over output words $y_t \in \mcV$:
\begin{equation}
p(y_t | y_{t-1}, \ldots, y_1, [h_1, \ldots, h_n]) = \boldW^{out}\boldc_t + b^{out}
\end{equation}
The models are trained to maximize the log probability of getting the sequences in the dataset correct. As the model is fully differentiable with respect to its parameters, we can train it end-to-end with stochastic gradient descent and the backpropagation algorithm.

We note that we have great flexibility in how our attention function $a$ combines the encoder context and the current decoder hidden state. In the next few sections, we explain standard choices for $a$ as well as our proposed coarse-to-fine attention model.

\section{Model 0: Standard Attention}

In \cite{bahdanau2014neural}, the function $a$ is implemented with an \emph{attention network}. We compute attention weights for each encoder hidden state $h_i$ as follows:
\begin{align}
\beta_{t,i} &= \boldh_i^\top \boldW^{attn} \boldh_t^{dec} \quad \forall i = 1, \ldots, n\\
\balpha_t &= \mathrm{softmax}(\bbeta_t) \\
\widetilde{\boldc}_t &= \sum_{i=1}^n \alpha_{t,i} \boldh_i
\end{align}
The idea behind attention is to select the most relevant words of the source (by assigning higher attention weights) when generating output word $y_t$ at time $t$.

The $\mathrm{softmax}$ function, defined as
$$\mathrm{softmax}([\beta_1, \ldots, \beta_n])_i =  \frac{\exp(\beta_i)}{\sum_{j=1}^n \exp(\beta_j)}$$
normalizes the $\alpha_i$ to sum to 1 over the source sentence words. This gives us a notion of probability distribution over the encoder words --- we can therefore write $\boldc_t$ as the expectation $\mathbb{E}_\alpha[\boldh]$, where we pick $\boldh_i$ with probability $\alpha_i$.

Our final context vector is then
\begin{equation}
\boldc_t = \tanh(\boldW^2[\widetilde{\boldc}_t, \boldh_t^{dec}])
\end{equation}
for $\boldW^2 \in \reals^{2d_{hid} \times d_{ctx}}$ a learned matrix.

Going forward, we call this this instantiation of the attention function \textsc{Model 0}.


\section{Model 1 and 2: Coarse-to-Fine Soft Attention}

For a large source input like a document, it may be computationally inefficient to run an RNN over the entire source. Instead, we can consider organizing the document into distinct sentences and select one sentence to attend to at a time.

Specifically, suppose we have sentences $s_1, \ldots, s_m$ with words $w_{i,1}, \ldots, w_{i,n_i}$ for sentence $s_i$, so that we can apply an RNN to each separately to get corresponding hidden states $\boldh_{i,j}$ for $i = 1, \ldots, m$ and $j = 1, \ldots, n_i$.
For attention, we then consider two options.

\paragraph{Model 1} We can follow \textsc{Model 0} and compute attention weights $\alpha_{i,j}$ for each hidden state $h_{i,j}$ by normalizing over all states. We call this \textsc{Model 1}.

\paragraph{Model 2} Alternatively, rather than taking attention over the entire document, we can instead have a two-layered hierarchical attention mechanism: first, we have weights $\alpha_1^s, \ldots, \alpha_m^s$ for each sentence, and then for sentence $s_i$, we have another set of weights $\alpha_{i,1}^w, \ldots, \alpha_{i,n_i}^w$.
Our final attention weight on word $w_{i,j}$ is then $\alpha_{i,j} = \alpha_i^s \cdot \alpha_{i,j}^w$.

In order to compute the sentence attention weights $\alpha_i^s$, we need to produce representations of each sentence; i.e., given the words $w_{i,1}, \ldots, w_{i, n_i}$ of the sentence, we produce a vector representation $\boldh^s_i \in \mathbb{R}^{d_{sent}}$.

Our first option is bag of words: we simply take the representation as
\begin{equation}
\boldh_i^s = \sum_{j=1}^{n_i} \boldE w_{i,j}
\end{equation}
i.e. the sum of the word embeddings, where $\boldE$ is another embedding matrix.

Alternatively, we can use a convolutional method: as in \citet{kim2014convolutional}, we perform a convolution over each window of words in the sentence using a fixed kernel width. We use max-over-time pooling to obtain a fixed-dimensional sentence representation in $\mathbb{R}^{d_f}$ where $d_f$ is the number of filters.

Explicitly, fix a sentence and suppose we have word vectors $\boldx_1, \ldots, \boldx_n$ with $\boldx_j = \boldE w_j \in \reals^{d_{in}}$, and suppose we have kernel width $k$ and convolution weights $\boldW^{conv} \in \reals^{d_f \times d_{in} k}$ where $d_f$ is the number of filters. Then applying the convolution
$\boldW^{conv} * [\boldx_1, \ldots, \boldx_n]$ gives result $\boldu = [\boldu_1, \ldots, \boldu_{n-k+1}]$ with $j$th element
$$\boldu_j = \boldW^{conv} \cdot [\boldx_j, \boldx_{j+1}, \ldots, \boldx_{j+k-1}] \in \reals^{d_f}$$
Our final output is given by 
\begin{equation}
\boldh^s = \mathrm{max-over-time}(\tanh(\boldu + b^{conv}))
\end{equation}
where $\mathrm{max-over-time}$ takes the maximum along the $j$-indexing dimension.


Thus, using the sentence representations, we can compute attention over the sentences.
For the words in each sentence, we run an LSTM over each sentence separately, and create attention weights over each sentence in the same way as \textsc{Model 0}. Using attention on word $w_{i,j}$ as  $\alpha_{i,j} = \alpha_i^s \cdot \alpha_{i,j}^w$, we can proceed exactly as in \textsc{Model 0} by computing the weighted average over hidden states $\boldh_{i,j}$.

We call this method of attention \textsc{Model 2}.

\section{Model 3: Coarse-to-Fine Sparse Attention}

With the previous models, we are required to compute hidden states over all words and sentences in the document, so that if we have $M$ sentences and $N$ words per sentence, the computational complexity is $O(MN)$ for each attention step.

However, if we are able to perform conditional computation and only compute on $M^+$ of the sentences, we can reduce the complexity to $O(M^+N)$. If we are able to make $M^+$ constant or even 1, this would give significant benefits to our overall complexity.

In our experiments, we will apply stochastic sampling to the attention distribution $\balpha$ in the spirit of \citet{xu2015captioning} and ``hard attention''.
Specifically, rather than computing the context $\widetilde{\boldc}$ as an expectation over $\balpha$ (i.e. $\boldc = \sum_{i=1}^n \alpha_i \boldh_i$), we can sample from the probability distribution $\balpha$ to obtain a single state $\boldh_i$, and we set $\widetilde{\boldc} = h_i$ as the sampled hidden state.

In our case, we take \textsc{Model 2} and apply hard attention at the sentence level, but keep the word level attention per sentence as is. That is, we sample from the attention weights $\alpha_1^s, \ldots, \alpha_m^s$ to obtain a one-hot encoding for the sentence attention, and apply the same multiplication with this one-hot vector on the word-level attention weights $\alpha_{i,1}^w, \ldots, \widetilde{\alpha}_{i,n_i}^w$ for all $i = 1, \ldots, m$. We call this \emph{Model 3}.


Because the hard attention model loses the property of being end-to-end differentiable, we turn to policy gradient methods from the reinforcement learning literature. %%%


\subsection{Practical considerations}

Recall the policy gradient update from section ??? at time $t$ of the decoder:
\begin{equation}
w \gets w + R_t \frac{\partial \log p(\balpha_t | y_1, \ldots, y_{t-1})}{\partial w}
\end{equation}
Since samples from $\balpha_t$ at time $t$ of the RNN decoder can also affect future rewards, we use a discount factor of $\gamma = 0.5$, so that the reward is $R_t = \sum_{s = t}^n \gamma^{n-s}r_s$ at time $t$, where $r_t = \log p(y_t | y_1, \ldots, y_{t-1})$ is the single step reward.

While this gradient is theoretically unbiased, because the gradient is sampled through a stochastic process, it tends to have high variance in practice. Thus, we implement a few variance reduction techniques to stabilize training.

\paragraph{Baseline reward} As described in section ???, we subtract a baseline from the reward to reduce variance while keeping the gradient unbiased.

In practice, we keep a separate baseline $b_t$ for each decoder time step $t$. Then our total discounted reward at time $t$ becomes
\begin{equation}
R_t = \sum_{s=t}^n \gamma^{n-s}(r_s - b_s)
\end{equation}

Our policy gradient update then becomes
\begin{equation}
w \gets w + R_t \frac{\partial \log p(\balpha_t | y_1, \ldots, y_{t-1})}{\partial w}
\end{equation}

There are a few methods for producing the baseline. We follow \citet{xu2015captioning} and keep an exponentially moving average of the reward for each time step $t$:
\begin{equation}
b_t \gets b_t + \beta (r_t - b_t)
\end{equation}
where $r_t$ is the average minibatch reward and $\beta$ is a learning rate (set to 0.1).

%Similarly, we keep a moving average of the variance for normalization:
%$$\sigma^2_j = (1 - \beta) \sigma^2_{j-1} + \beta v_j$$
%where $v_t$ is the variance of the minibatch rewards for batch $j$. Since we have rewards at each time step of the decoder LSTM, we keep a separate moving average for the baseline for each time step, but we keep a single moving variance for all time steps.

While several papers suggest using a learned baseline from the RNN state in RL tasks  \citep[e.g.][]{mnih2014visualattention, ranzato2015}, we have not found this to be effective. In our experiments, we found that attempting to learn the baseline failed to converge, most likely because there is not enough correlation between the RNN state and the reward.


In addition to including a baseline, we also scale the rewards by a tuned hyperparameter --- we found that scaling helped to stabilize training. We empirically set this scale to 0.3.

\paragraph{Randomly training using soft attention} \citet{xu2015captioning} explain that training hard attention with REINFORCE has very high variance, even when including a baseline. Thus, for every minibatch of training, they randomly use soft attention instead of hard attention with some probability (they use 0.5).
The backpropagated gradient is then the standard soft attention gradient instead of the REINFORCE gradient.

While this method helps stabilize training, it's aesthetically not very pleasing. Our goal in coarse-to-fine attention is to reduce computation over the encoded hidden states, while this method requires that we perform the full amount of computation as soft attention for a random subset of minibatches. However, we include this training method in our experiments to test how feasible hard attention is.

%We also include an entropy-increasing term to encourage exploration during the training process, which is important to speed up convergence of learning. Our policy gradient equation then becomes
%\begin{equation}
%w \gets w + (R_t - b) \frac{\partial \log p(\balpha_t | y_1, \ldots, y_{t-1})}{\partial w}
% + \lambda_{ent} \frac{\partial H(\balpha_t)}{\partial \balpha_t}
%\end{equation}
%where $H(\balpha_t) = -\sum_{i=1}^n \alpha_t \log \alpha_t$ is the entropy function and $\lambda_{ent}$ is a hyperparameter.


%\subsection{Multiple Samples}
%
%From our initial experiments with Model 3, we found that taking a single sample was not very effective. However, we discovered that sampling multiple times from the distribution $\alpha$ significantly improves performance.
%
%We sample based on the multinomial distribution $\mathrm{Mult}(k, \{\alpha_i\}_{i=1}^n)$ to produce the sentence-level attention vector $\alpha$ of length $n$, with $\alpha_i = x_i / k$, where $x_i$ is the number of times index $i$ was sampled. $k$ is a hyperparameter which can be tuned, and we found that $k=5$ works well in our experiments.
%
%The intuition here is for the hard attention model to more closely approximate the soft attention model, as it can select more sentences to produce the context.

%\subsection{Curriculum}
%
%Since training using policy gradients tends to be noisy and slow to converge, we experimented with a curriculum that starts training with soft attention and in epoch $t$, trains a minibatch using hard attention with probability $p_t = 1 - 1/\sqrt{t}$ \citep{bengio2016hardntm}.
%
%While we found this to be helpful for single sample hard attention, it was not necessary for effective training with multisampled hard attention. We prefer to train solely with hard attention when possible, as we are able to save computation at training time.

%\section{Sparsemax}
%
%% describe


\chapter{Experiments}

\section{Data}
% if we do other experiments, include more here.

\subsection{CNN/Dailymail}

Experiments were performed on the CNN/Dailymail dataset from \cite{Hermann2015}. While the dataset was created for a question-answering task, the dataset format is suited for summary. Each data point is a news document accompanied by up to 4 ``highlights'', and we take the first of these as our target summary. Train, validation, and test splits are provided along with document tokenization and sentence splitting. We do additional preprocessing by replacing all numbers with \# and appending end of sentence tokens to each sentence. We limit our vocabulary size to 50000 most frequent words, replacing the rest with \texttt{<unk>} tokens. We dropped the documents which had an empty source (which came from photo articles).

 Table~\ref{data_stats} lists statistics for the CNN/Dailymail dataset.

% dataset statistics
\begin{table}[h]
\centering
\begin{tabular}{lcc}
\toprule
Dataset  & CNN & Dailymail \\
\midrule
Train size & 90267 & 196962 \\
Valid size & 1221 & 12149 \\
Average words per doc & 794 & 832\\
Average sents per doc & 21 & 29\\
Average words per sent & 36 & 27\\
Average words per summary & 13 & 15 \\
\bottomrule
\end{tabular}
\caption{Statistics for CNN/Dailymail data.}
\label{data_stats}
\end{table}

% examples of data needed
\todo{examples of data}


%\section{Synthetic Pretraining} % are we still doing this???
%
%We found that unsupervised pretraining on the given dataset is beneficial to learning. For each document, we randomly sample 2 sentences and concatenate to form the target sentence in a new synthetic dataset. We can sample multiple times to have multiple targets for a given source document, and we found that 5 samples was most beneficial to learning (performance drops with significantly more samples). We then train on the synthetic dataset for 5 epochs and initialize future training with the learned weights.

\section{Implementation details}

A few implementation details were necessary to make minibatch training possible. First, instead of taking attention over each individual sentence, we arrange the first 400 words of the document into a 10 by 40 image, and take each row to be a sentence. Second, we pad short documents to the maximum length with a special padding word, and allow the model to attend to it. However, we zero out word embeddings for the padding states and also zero out their corresponding LSTM states. We found in practice that very little of the attention ended up on the padding words.

Ideally, we would prefer to not truncate documents, especially since later context can be important for summarizing the document. Due to memory issues, this is a problem we still have to resolve.

\section{Models}

\subsection{Baselines}

For a baseline, we take the first 15 words of the document (chosen as the average length of a sentence in the training dataset). We call this \textsc{First}.

Others...
% Need way more here...
% ideas: berkeley system (if it works), look for others - sentence fusion?


\subsection{Our models}

We ran experiments with Models 0 to 3 as described above. Model 0 serves as the baseline.

\begin{itemize}
\item Model 0: Soft attention.
\item Model 1: Organized by sentence, soft attention over all.
\item Model 2: Hierarchical LSTM, coarse-to-fine with soft attention.
\item Model 3: Hierarchical LSTM, coarse-to-fine with hard attention over sentences.
%\item Model 3+multisampling: We include multisampling with $k=5$.
\end{itemize}



\section{Training}

%% fix batch size and other details, and also note we train until convergence (which is slightly longer for hard attn)
We train with minibatch stochastic gradient descent (SGD) with batch size 20 for 20 epochs, renormalizing gradients below norm 5. We initialize the learning rate to 0.1 for the sentence encoder and 1 for the rest of the model, and begin decaying it by a factor of 0.5 each epoch after the validation perplexity stops decreasing.\footnote{We tried more complicated optimization methods such as Adagrad \citep{Duchi2011} and Adam \citep{Kingma2015}, but found that they did not perform as well. This could be due to gradient norms that are too large.}

We use 2 layer LSTMs with 500 hidden units, and we initialize word embeddings with 300-dimensional word2vec embeddings \citep{mikolov2013word2vec}. For convolutional layers, we use a kernel width of 6 and 600 filters.

% talk about LSTM soft attention?

%\paragraph{Hard attention}
%Rewards for multisampling are scaled by a factor of $0.04$ (in addition to scaling by the moving variance).



Our models are implemented using Torch \citep{Torch} based on a past version of Harvard's OpenNMT system\footnote{https://github.com/harvardnlp/seq2seq-attn}. We ran our experiments on a GPU (specs?).

In the next chapter we show results.

\chapter{Results}

\section{Evaluation}

We report metrics for best validation perplexity and ROUGE scores \citep{lin2004rouge}. We use the ROUGE balanced F-scores with ROUGE-1 (unigrams), ROUGE-2 (bigrams), and ROUGE-L (longest common substring). We chose F-score since recall is biased towards longer predicted sentences.

To generate summaries for evaluation, we run beam search with a beam size of 5.



\begin{table}[h]
\centering
\begin{tabular}{llcccc}
 \toprule
 Model &  & PPL & ROUGE-1 & ROUGE-2 & ROUGE-L \\
 \midrule
\textsc{First} & & - & 23.1 & 9.8 & 20.5 \\
\textsc{Berkeley} \\
\textsc{Model 0} & & 13.6 &  \\
 \textsc{Model 1} & & \\
\textsc{Model 2 conv} & & 15.8 & & \\
\textsc{Model 2 sparsemax} & & \\
\textsc{Model 3 hard only}  \\
\textsc{Model 3 semi sampling}  \\
\midrule
\textsc{Model 2 BOW} & & 16.4 & & \\
\textsc{Model 2 LSTM}  & & \\
\textsc{Model 2 5x80 conv} \\
\textsc{Model 2 2x200 conv} \\
 \bottomrule
\end{tabular}
\caption{Summarization results for CNN/Dailymail.}
\label{table:summary}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{llr}
\toprule
Model & & Entropy \\
\midrule
\textsc{Model 0} \\
\textsc{Model 1} \\
\textsc{Model 2 conv} \\
\textsc{Model 2 BOW} \\
\textsc{Model 2 conv +entropy}  \\
\textsc{Model 2 sparsemax} \\
\bottomrule
\end{tabular}
\caption{Entropy over sentence attention. All numbers are for  We computed \textsc{Model 0} and \textsc{Model 1} entropy by summing over each row.}
\label{table:entropy}
\end{table}

\section{Analysis}

See Table~\ref{table:summary} for summary results.

We investigate the entropy of the sentence attention on the validation set in Table~\ref{table:entropy}.

% synthetic data

% MT?

% training curves
We see hard attention takes longer to converge.

%% attention figures 
We see that the attention is very spread out over sentences. In particular, we notice that the word-level attention focuses on specific stop words for all decoder time steps. We posit this may be due to ???
See Appendix~\ref{appendix:attn} for more visualizations.

% predicted examples
We show some predicted summaries from the model.



\chapter{Conclusion}


\bibliographystyle{plainnat}
\bibliography{../papers/Mendeley/library}


\begin{appendices}

\chapter{Attention Visualizations}
\label{appendix:attn}
hi

\end{appendices}

\end{document}
