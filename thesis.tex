\documentclass[11pt]{report}
\usepackage{common}
\usepackage[round]{natbib}
\usepackage[strings]{underscore}
\usepackage{url}
\usepackage{todonotes}

\title{Document Summarization (draft)}
\author{Jeffrey Ling}
%\date{}
\begin{document}
\maketitle{}

\begin{abstract} % needs work
Standard sequence-to-sequence (seq2seq) attention models have seen great success in NLP, but do not scale well to tasks with long sequences. We propose a novel coarse-to-fine attention method to reduce the number of computations necessary in standard attention. By organizing the source sequence into a 2-dimensional image, we hierarchically apply attention, using a coarse mechanism for the first layer to select a row sequence, and a finer mechanism for the second soft attention layer. While the computation for training standard seq2seq models scales linearly with source sequence length, our method is invariant to length and thus can scale arbitrarily.

We evaluate our model on the CNN/Dailymail document summarization task.
\end{abstract}

\tableofcontents{}




\chapter{Introduction}

\section{Natural Language Processing}

Natural language processing is a field with a variety of interesting structured prediction problems. The essential goal of NLP is to build a model of language so that computers can automatically process substantial quantities of text --- a highly relevant problem in today's information age.

While humans have no trouble understanding and using language, even the simplest language tasks can be impossible for computers. Some classical NLP problems include part-of-speech tagging, parsing, language modeling, and machine translation. %%

It is informative to consider the history of machine translation. The first methods were rule-based linguistics systems, and the company SYSTRAN provided one of the first \citep{}.

In the 90s, statistical methods for NLP became increasingly popular. By considering language generation as a probabilistic process, one can collect statistics over large corpuses of data to automatically deduce the parameters of the model. In machine translation, this idea was used to great effect in \citet{Brown1993}, whose count-based methods form the core of state-of-the-art systems like Google Translate.

Recently, researchers found that deep learning works effectively for many NLP tasks. For the first time, neural networks were able learning structure and features from language almost completely from scratch \citep{Collobert2011}. % \cite{Bengio2003} language model
The success of neural methods has been adopted by Google Translate to build even better systems \citep{GoogleTranslate2016}. Research in applying deep learning to NLP is ongoing.

\section{Deep Learning}

The history of neural networks dates back to the perceptron \citep{Rosenblatt1958}, a simple model that assumes data can be linearly separated. Due to this strict requirement, the machine learning community dismissed the idea as impractical, and research was sidelined for most of the 20th century.

%While some work continued \citet{Rumelhart1986} showed that the backpropagation algorithm could be used to efficiently train neural networks, 

Recently, neural networks have made a resurgence. In the ImageNet image classification competition in 2012, \citet{Krizhevsky2012} won using \emph{convolutional neural networks} \citep{LeCun1995}, beating the competition by a significant margin. This led to a renewed wave of research, especially due to the advancement of modern computing power and GPUs, which can train networks at 10 or 20 times the speed of standard CPUs. Today, deep models are used to successfully play Go \citep{Silver2016}, play Atari games from pixels \citep{deepatari2015}, and %???

%%% break

While neural networks are often treated as black box classifiers, \citet{Zeiler2014} show that the intermediate layers of deep convolutional networks contain abstracted qualities of the input, such as patterns, textures, and objects of the input. This suggests that neural networks are discovering features of the input and building generalized \emph{representations} of their inputs.

% image?

The idea of learning representations of the input is highly general, and so it comes as no surprise that deep networks soon found applications in NLP. \citet{mikolov2013word2vec} show that by training a neural network on a Google News text corpus, the network learned to map words in the English language to a vector of real numbers known as \emph{word embeddings}. These word embeddings are actually able to capture semantic properties of the words --- for example, taking the vectors for \emph{king}, \emph{man}, and \emph{woman}, we find that $v_{king} - v_{man} + v_{woman} \approx v_{queen}$, preserving the analogy that we usually make with text.

% diagram of vectors

Since the onset of deep learning, deep models have found their way into nearly every corner of NLP. Much of their success relies on the ubiquity of the \emph{long short-term memory} (LSTM) recurrent neural network \citep{hochreiter1997long}, a model used to both process and generate sequences of text. State-of-the-art systems for many problems, including machine translation, now use deep learning as their core algorithm. %???

% While there is much to cover  \cite{goldberg2015primer} provides a nice summary.


\section{Motivation}

% should this go under deep learning?
% this could be a hard section to write

Why deep models? There is a debate in the deep learning community about the extent to which human inductive biases ought to be incorporated into deep networks. On one hand, machine learning systems often incorporate handcrafted features or assumptions in the model to solve the problem. Examples % references? c

On the other hand, a purely data-driven system forgoes these details, using highly general models for the task at hand.

In the past, NLP was done with handcrafted features. However, \citet{Collobert2011} shows that this is an unnecessary assumption. They build a neural network for classical NLP tasks (e.g. part-of-speech tagging, named entity recognition) that can learn to do the task without any feature extraction.


There are a few reasons why deep networks are desirable for NLP. First, they are remarkably powerful and practically easier to train than complicated rule-based systems (assuming enough computational power is available), and are not mutually exclusive with standard feature extraction methods.
Second, they allow for non-domain experts to make progress on relevant problems.
Third, we find that trained models can discover latent structure in language automatically, which is an interesting phenomenon that may reveal more about how language is used. As an example, sequence-to-sequence models with attention \citep{bahdanau2014neural} learn the concept of a word alignment in translation without any supervision.

It is not yet clear, however, to what extent neural methods can replace models with hard-coded linguistic assumptions. This is one of the questions we set out to resolve.


\section{Our problem}

Text summarization is an important problem for compressing large bodies of natural text into a more easily digestable form. Document summarization is one of the most challenging formulations of this problem, where given a document with several sentences of text, the goal is to produce a coherent summary that captures most or all of its salient points.

To accomplish this, we use the most recent advances in deep learning to automatically produce summaries by training on a large dataset of given examples.

We can frame summarization as a supervised sequence-to-sequence task: given a news article (the source sequence of words), we desire to produce a summary (the target sequence). Existing methods in deep learning have been developed and proven to be highly effective for this kind of task, especially the sequence-to-sequence (seq2seq) model applied to machine translation \citep{sutskever2014sequence, bahdanau2014neural}. \citet{rush2015neural} uses the seq2seq model to summarize sentences into shorter headlines, but we consider the more general problem of arbitrarily long documents.

Existing seq2seq methods are limited by the length of source and target sequences. For a problem such as document summarization, the source sequence of length $N$ requires $O(N)$ seq2seq model computations. However, it makes sense intuitively that not every word of the document will be necessary for generating a summary, and so we would like to reduce the amount of necessary computations over the source document.

Therefore, in order to scale seq2seq methods for this problem, we aim to prune down the length of the source sequence in an intelligent way. In the problem of image captioning, \citet{xu2015captioning} show that when generating words of the caption, the model places attention on the relevant part of the image.
Inspired by this idea, we introduce the coarse-to-fine attention mechanism that sparsely selects subsets of the source document for processing.


We provide an outline for the rest of this thesis.
In section ? we cover related work in summarization and deep learning.
In section ? we give the necessary background for our models.
In section ? we describe our models in detail.
In section ? we show results and discussion.



%%%%%%%%%%

\chapter{Related Work}

\section{Automatic Summarization}

\citet{Nenkova2011} give an overview of the field. In particular, they provide a taxonomy of methods that researchers have used to frame summarization:

\paragraph{Extractive vs. Abstractive} Extractive summaries extract certain sentences or phrases from the document, while abstractive summaries take a more holistic view and can in practice be anything (similar to how humans produce summaries). 

\paragraph{Single- vs. Multi-document} Summarization was originally posed as the problem of producing a summary for a single document. However, with the onset of the Internet and search engines, there are often multiple documents on the same topic, and so a summarization system should be able to use all of them to produce a summary.

\paragraph{Indicative (style) vs. informative (facts)} Indicative summaries provide a sense for what a certain document is about without necessarily giving all of its details, while informative summaries are meant to replace the original document in terms of content.

\paragraph{Keyword (words) vs. headline (sentence)} Keyword summaries are allowed to simply be a bag-of-words of important keywords from the document, while headline summaries must form a coherent sentence.

\paragraph{Generic vs. query focused} Generic summaries have no assumptions on the reader and are meant to be generally informative, while query focused summaries take into consideration a query and only return relevant information. The contrast between these two methods highlights an important question in summarization: to what end are we summarizing documents? If we can answer this question more precisely, we will be better able to build systems to accomplish our desired tasks.

\vspace{0.5cm}

Before elaborating on specific methods, it is important to highlight these differences to understand how people thought about the problem. For example, extractive methods are by far more popular due to the simplicity of building an extraction algorithm, while abstractive methods have been difficult as we do not have a fully working model of language generation.

Based on this taxonomy, the deep model we set out to build would be classified as 1) abstractive, 2) single-document, 3) informative, 4) headline, 5) generic.

In the rest of this thesis, we will limit the scope of the summarization problem to the \emph{single-document}, \emph{informative}, \emph{headline}, and \emph{generic} categories in order to focus on the application of deep learning in the problem.

In the next section, we give a brief overview of some of the relevant methods used in summarization.

\section{Methods}



\subsection{Classical}

One of the first considerations of automatically producing summaries was \cite{luhn1958automatic}, which aimed to summarize scientific articles using a sentence-ranking method. The algorithm gives each sentence a score based on the occurrence of frequently appearing words.


Since then, a variety of approaches have been used to solve summarization. We highlight some notable work in both the extractive and abstractive framework.

\subsubsection{Extractive}

The most popular methods for document summarization have generally been extractive due to their simplicity. One natural procedure for an extractive summarization is to score sentences based on some relevance metric and return the highest scoring sentences (perhaps reordering them as well).

Some examples are \cite{carbonell1998MMR}, which uses a simple information metric for ranking sentences, and \cite{svore2007}, which uses a simple neural network for the same purpose.

\citet{Shen2004} models sentence extraction as a sequential decision problem, using a linear-chain conditional random field to find the best subset of sentences.



\subsubsection{Abstractive}

While extraction has proven to be successful, the method is inherently limited in its ability to summarize. The more challenging method, and also the closest to what humans do, is \emph{abstractive} summarization. Instead of strictly requiring that all words of the summary come from the source document, any coherent text is allowed.

Two methods used to produce abstractive summaries are sentence compression and sentence fusion. Compression removes less useful information from sentences, while fusion is harder and combines information from sentences. % cite Barzilay, MCkeown

Compression: \citet{knight2002summarization} employs a noisy channel model, similar to machine translation, to deduce the ``most probable'' compression, while \citet{clarke2008global} uses an integer linear program. \citet{cohn2008sentence} extend the tree-based methods to allow for insertions and substitutions during compression, whereas prior methods were purely deletion based. \cite{zajic2004topiary} successfully use a sentence compression algorithm along with an unsupervised topic model on the DUC 2004 task.

Fusion: align parse trees and combine phrases that are similar % more needed


\subsection{Deep Learning}

With the onset of deep learning, learning an end-to-end abstractive model for summarization has become more feasible. \citet{rush2015neural} propose a data-driven, completely abstractive model for summarizing short sentences by training a sequence-to-sequence model with attention. More recent work in deep learning has been done for both extractive \citep{} %% citation needed
and abstractive \citep{nallapati2016seq2seq, ramachandran2016} methods that scale the models to full documents, demonstrating the feasibility of end-to-end models.

These new models require a large amount of supervised training data, which the DUC data are unsuitable for due to their small scale. However, thanks to the large-scale annotated CNN/Dailymail news stories dataset \cite{Hermann2015}, we now have the necessary data to train our deep models.

The task has not yet been fully standardized in this context, and research in the area is still largely preliminary. While CNN/Dailymail may not be the most suitable dataset for the task \citep{Chen2016}, a better alternative is yet to exist.


\subsection{Datasets}

To standardize the task, NIST released data for DUC (Document Understanding Conferences) between 2001-2007 \citep{over2007duc}. The DUC tasks involved producing summaries for both single- and multiple-document sets of news articles. DUC 2001 and 2002 ask for general summaries of these articles documents and summaries, while DUC 2003-2006 also evaluate summaries based on their usefulness for certain question-answer tasks. % more details?

While a single most effective metric for summarization may not exist, the DUC conferences established several important criteria, including grammaticality, non-redundancy, and content coverage (for which metrics like ROUGE \citep{lin2004rouge} were created).

DUC overall was found not to have the best impact. For many of the news summary tasks, it was found that taking the first sentence of each article could not be beaten by more sophisticated methods, and so the task was somewhat abandoned.

CNN/Dailymail is much larger, and is the one we use to train our deep models.


\subsection{Evaluation}

Evaluating a good summary is inherently ambiguous, and probably one of the hardest parts of the problem.

For extractive summaries, people have proposed simple metrics such as precision and recall on selected sentences. These naturally do not work too well since 1) not all sentences are equally informative, and 2) not all parts of a sentence are relevant.

DUC really pushed forward understanding on evaluation. They came up with recall on elementary discourse units (EDUs), based on clauses within a summary that ought to be captured. ROUGE \cite{lin2004rouge} is cheap and fast. Pyramid method is a complicated human evaluation method based on summary content units (SCUs). 

None of these methods directly address the grammaticality of the output. Aside from using human evaluation, meaningful metrics for summaries is still very much an open question \citep{toutanova2016summarymetrics}. In our work, we settle for ROUGE due to its cheapness and ease of use in evaluating our models.


\subsection{In the wild}

Summarization is an important real-world problem due to the explosion of available data. Thus, there are many practical methods that have been developed and deployed in real-world settings. One of the most notable examples is on Reddit\footnote{\url{reddit.com}}: in order to summarize long forum discussions, Reddit uses technology from \textsc{smmry}\footnote{\url{smmry.com}}.

Smmry's algorithm is a simple extractive summarization method. It counts word occurrences, splits discussions by sentence, and ranks the sentences based on the sum of their word scores (perhaps tf-idf?). This algorithm bears extraordinary similarity to \citet{luhn1958automatic} --- although a variety of methods have been invented since then, the simplest approaches turn out to be the most practical.

% Other examples include a SUMMLY app thing acquired by Yahoo - it had a lot of hype but it's unclear if it had any real content.


%%%%%%%
\chapter{Background}

In this chapter, we set up the relevant background ideas for our models.

\section{Sequence-to-Sequence Attention Models}

The sequence-to-sequence architecture \citep{sutskever2014sequence}, also known as the encoder-decoder architecture, forms the backbone of many successful models in NLP. A popular variant of sequence-to-sequence models are \emph{attention} models \citep{bahdanau2014neural}. The key idea is to keep an encoded representation of all parts of the input, attending to the relevant part each time we produce an output from the decoder.
These models have been used to great effect in a variety of NLP tasks, including machine translation \citep{sutskever2014sequence, bahdanau2014neural}, question answering \citep{Hermann2015}, dialogue \citep{li2016persona}, caption generation \citep{xu2015captioning}, and in particular summarization \citep{rush2015neural}.

\citet{xu2015captioning} show how attention models can be used to ``summarize'' an image and produce a caption. 
By analyzing where in the image their models attend to when generating each word of the caption, they qualitatively find that the model is essentially describing that region of the image. 
Figure~\ref{} shows some examples.

% image?

We can leverage the same idea for text summarization, assuming we have a suitable representation of our input document. The simplest method for doing so would be to run an LSTM over the document.

However, the attention step becomes computationally difficult --- for each word we generate, we need to compare it to every word of the document in order to determine which part to attend to. Therefore, we propose a hierarchical method of attending to the document by first attending to sentences, then to the words within sentences. We call this method \emph{coarse-to-fine attention}
\footnote{The term coarse-to-fine attention has previously been introduced in the literature \citep{mei2016}. However, their idea is different: they use coarse attention to reweight the fine attention computed over the entire input. Similar ideas have also been called hierarchical attention \citep{nallapati2016seq2seq}.}.



To be able to attend to both sentences and words in a hierarchical manner, we need to construct encodings of the document at both levels. Thus, we run a low-level LSTM encoder on the words of each sentence for a fine-grained representation of the text, and a simpler encoder model (e.g. bag of words) for coarse-grained sentence representations.
\citet{Sukhbaatar2015} demonstrate how coarse representations can be useful by using memory networks to access information for simple question-answering tasks.
\citet{li2015autoencoder} use the idea of coarse and fine encodings to develop a hierarchical autoencoder for representing paragraphs of text.
%by using low-level LSTMs on words to build sentence representations, then a high-level LSTM on the sentences for the final paragraph representation.

Therefore, if we can make our model first use coarse attention to choose sentences, then use fine attention to choose words only from that sentence, then we avoid the computational cost of searching over the entire document. This idea runs into a very serious problem, however: by posing the attention as a discrete selection process, the neural network is no longer differentiable.


\citet{xu2015captioning} suggest ``hard'' attention as one possible solution to the discrete selection problem. While standard ``soft'' attention actually averages the representations of where the model attends to, for hard attention we make a hard decision and choose only one location. Such models can be trained using reinforcement learning. Before we elaborate on this method, we survey some other methods invented to overcome this discrete attention problem.

\section{Conditional Computation}


%The problem of full document summarization, however, is still very open. In order to make the models work, \cite{nallapati2016seq2seq} use a variety of tricks: limiting the decoder vocabulary to the document, the use of pointer attention for \texttt{<unk>} tokens, etc. Our goal in this paper is not necessarily to optimize performance, but to understand how to scale up existing seq2seq methods in an efficient way, and so we attempt to eliminate the use of these ad-hoc tricks whenever possible. We aim to find the best implementation of ``hard'' attention, in which a discrete subset of the source document is selected at any given time, to satisfy the scalability condition.

\todo{elaboration needed}

Many techniques have been proposed in the literature to handle the problem of large inputs to deep neural networks.

The term ``conditional computation'' was coined by \citet{BengioLC13}, where the idea is to compute a subset of units for a given network per input. This would have the advantage of being much more efficient, especially for networks that need to handle extremely large inputs as is common in vision and NLP.

While standard deep models use the softmax function $\mathrm{softmax}(\boldz)_i = \exp(z_i) / \sum_j \exp(z_j)$ to produce differentiable probability distributions, \citet{martins2016sparsemax} propose the sparsemax function as a sparse alternative to softmax that still has a useful gradient.

\citet{rae2016sparsememory} use a nearest neighbors approach in ``sparse access memory'' to train a large-scale neural Turing machine.

\citet{Shazeer2017} introduces a mixture-of-experts model that selectively chooses some ``expert'' subnetworks in the spirit of conditional computation.

These methods are all promising and merit further research in the future. In this work, we limit our scope to the hard attention model of \citet{xu2015captioning}. In the next section, we explain reinforcement learning and how it can be used to train the hard attention model.

\section{Reinforcement Learning}

Standard backpropagation training of neural networks assumes that the output is a differentiable function of the input. Reinforcement learning, however, is a general framework that makes no such assumptions.

The traditional setup of reinforcement learning assumes some agent is navigating an environment and earning rewards. We assume the agent is at state $s_t$ in time $t$, makes an action $a_t$, earns a reward $r_t$, and transitions to the next state $s_{t+1}$. Assuming there is randomness in the reward and the transition $s_{t+1} \sim p(s_{t+1} | s_t, a_t)$, the agent wants to maximize total expected reward $\mathbb{E}_{s_t, r_t} [\sum_{t=0}^\infty \gamma^t r_t]$ where $\gamma$ is a future discount factor.

% diagram

It turns out that such a framework can be applied to deep neural networks to obtain gradients for backpropagation. 

%by backpropagating an unbiased gradient at the action step, which is commonly known as the REINFORCE algorithm or policy gradient \citep{williams1992reinforce, schulman2015backprop}.

%%%%% 
\todo{transition}

To train Model 3, we must apply techniques from the reinforcement learning literature. We cast our learning problem with the neural network as stochastic agent and log probability as reward, and thus are able to apply reinforcement learning to train the network.

In our setup, where our agent is a parameterized model, computing the gradients for the model in this setup is known as the REINFORCE algorithm \citep{williams1992reinforce} or policy gradient, and has been well-explained in recent work \citep{mnih2014visualattention, ba2015visualattention, schulman2015backprop}.

Specifically, assuming we have a probability distribution $p(\alpha)$ from which we sample, and subsequently receive reward $r$, the gradient that is backpropagated from $p(\alpha)$ is
\begin{equation}
\label{policygrad}
\frac{\partial \mcL}{\partial w}  = r \frac{\partial \log p(\alpha)}{\partial w}
\end{equation}
i.e. the reward multiplied by the gradient of the log probability. A proof of this can be found in \cite{williams1992reinforce}.

In our framework, we use the log probability of the correct word at each time step as the reward $r_t$. Since samples at time $t$ of the RNN decoder can also affect future rewards, we use a discount factor of $\gamma = 0.5$, so that the reward is $r = \sum_{s = t}^n \gamma^{n-s}r_s$ for the decoding sampler at time $t$.

\subsection{Variance Reduction}

While the policy gradient of equation~\ref{policygrad} is proven to be unbiased, in practice it has such high variance that training converges far too slowly.

One of the most common ways to reduce the variance of the gradient estimator is to introduce a baseline reward $b$, which we subtract from our reward. Including a baseline is proven to reduce the variance of the estimator \citep{mnih2014belief}. We also normalize the rewards to a common scale by dividing by the reward variance in a given minibatch.


Our policy gradient equation then becomes
\begin{equation}
\label{policygradbaseline}
\nabla_\theta \mathcal{L}(\theta) = \frac{r-b}{\sigma} \nabla_\theta \log p(\alpha)
\end{equation}

There are a few methods for producing the baseline; as in \cite{mnih2014belief}, we can keep an exponentially moving average of the reward
$$b_j = (1 - \beta) b_{j-1} + \beta r_j$$
where $r_j$ is the average minibatch reward and $\beta$ is a hyperparameter (set to 0.9).
Similarly, we keep a moving average of the variance for normalization:
$$\sigma^2_j = (1 - \beta) \sigma^2_{j-1} + \beta v_j$$
where $v_t$ is the variance of the minibatch rewards for batch $j$. Since we have rewards at each time step of the decoder LSTM, we keep a separate moving average for the baseline for each time step, but we keep a single moving variance for all time steps.

While several papers suggest using a learned baseline from the RNN state (e.g. \cite{ranzato2015}), we have not found this to be more effective.

We also use an entropy term to reduce the variance. Our policy gradient equation then becomes
\begin{equation}
\label{policygradbaselineentropy}
\nabla_\theta \mathcal{L}(\theta) = \frac{r-b}{\sigma} \nabla_\theta \log p(\alpha) - \lambda_{ent} \nabla_\alpha(\alpha \log \alpha)
\end{equation}
where $\lambda_{ent}$ is a hyperparameter. This has the effect of increasing the entropy of our sampling distribution, hence encouraging more exploration and faster convergence of learning.


\subsection{Deep reinforcement learning}
%% deep learning in vision: \cite{deepatari2015}

Deep reinforcement learning has been tried in the context of NLP \citep{zaremba2015rlntm, ranzato2015, li2016dialogueRL} with varying degrees of success so far. We experiment with this in our paper.

%% TODO: new work here. \cite{Yogatama2017, xu2015captioning, narasimhan2016, andreas2016}, ... missing a lot here

% explain policy gradient here

Inspired by these ideas, we aim to improve on previous work for document summarization by (1) strengthening the hierarchical assumption with coarse features at the sentence level, and (2) including reinforcement learning for sparse attention.

In the next chapter we describe our models in detail.



\chapter{Models}

In this chapter, we describe our models in full mathematical detail.

\section{Sequence-to-sequence (seq2seq)}

We first describe the neural network architecture of the seq2seq models, also known as encoder-decoder models.

\subsection{Recurrent encoder and decoder}

As described in \cite{bahdanau2014neural}, an \emph{encoder} recurrent neural network (RNN) reads the source sequence as input to produce a vector known as the \emph{context}, and a \emph{decoder} RNN generates the output sequence using the context as input.  One popular RNN choice is the long-short term memory (LSTM) network \citep{hochreiter1997long}.

% more specifics here?
More formally, a given sentence $w_1, \ldots, w_n \in \mathcal{V}$ is transformed into a sequence of vectors $x_1, \ldots, x_n \in \mathbb{R}^{d_{in}}$ through a word embedding matrix $E \in \mathbb{R}^{|\mathcal{V}| \times d_{in}}$ as $x_t = Ew_t$. An RNN is given by a parametrizable function $f$ and a hidden state $h_t \in \mathbb{R}^{ d_{hid}}$ at each time step $t$ with $h_t = f(x_t, h_{t-1})$. This sequence of hidden states $h_1, \ldots, h_n$ jointly forms the context $c_{enc} = [h_1, \ldots, h_n]$ that is passed to the decoder.

%% diagram

The decoder is another RNN $f_{dec}$ that generates output words $y_t \in \mathcal{V}$. It keeps hidden state $h_t^{dec} \in \mathbb{R}^{d_{hid}}$ as $h_t^{dec} = f_{dec}(y_{t-1}, h_{t-1}^{dec})$. Each word is predicted using another function $g$ as
$$p(y_t | y_{t-1}, \ldots, y_1, c) = g(h_t^{dec}, c_{enc})$$
The models are trained to maximize the log probability of getting the sequences in the dataset correct. As the model is fully differentiable with respect to its parameters, we can train it end-to-end with stochastic gradient descent and the backpropagation algorithm.

The details of the function $g$ will be described next.

\subsection{Model 0: Standard Attention}

In \cite{bahdanau2014neural}, the function $g$ is implemented with an \emph{attention network}. We compute attention weights for each encoder hidden state $h_i$ as follows:
$$\beta_i = h_i^T W h_t^{dec}$$
$$\alpha_i = \frac{\exp(\beta_i)}{\sum_{j=1}^n \exp(\beta_j)}$$
$$\widetilde{c} = \sum_{i=1}^n \alpha_i h_i$$
We normalize the $\alpha_i$ to sum to 1 over the source sentence words.

$g$ is then implemented as
$$g(h_t^{dec}, c) = W_{out}(\widetilde{c}^T W_2 h_t^{dec}) + b_{out}$$

In essence, $g$ computes a probability distribution $\alpha$ over the encoder hidden states, then takes the expectation of the encoder hidden state under $\alpha$. The idea behind attention is to select the most relevant words of the source (by assigning higher attention weights) when generating output word $y_t$ at time $t$.

Going forward, we call this \emph{Model 0}.

\subsection{Model 1 and 2: Coarse-to-Fine Soft Attention}

For a large source input like a document, it may be computationally inefficient to run an RNN over the entire source. Instead, we can consider organizing the document into distinct sentences and run an RNN separately over each. Specifically, assuming we have sentences $s_1, \ldots, s_m$ with words $w_{i,1}, \ldots, w_{i,n_i}$ for sentence $s_i$, we can apply an RNN to get corresponding hidden states $h_{i,j}$.

For attention, we then have two options. We can follow Model 0 and compute attention weights $\alpha_{i,j}$ for each hidden state $h_{i,j}$ by normalizing over all states. We call this \emph{Model 1}.

Alternatively, rather than taking attention over the entire document, we can instead have a two-layered hierarchical attention mechanism: first, we have weights $\alpha_1, \ldots, \alpha_m$ for each sentence, and then for each sentence $s_i$, we have another set of weights $\widetilde{\alpha}_{i,1}, \ldots, \widetilde{\alpha}_{i,n_i}$.

The sentence level attention is computed using a different method: we first produce a representation of each sentence given the words $x_{i,1}, \ldots, x_{i, n_i}$ of the sentence. Our first option is \emph{bag of words}: we simply take the sentence representation $u_i = \sum_{j=1}^{n_i} Ex_{i,j} \in \mathbb{R}^{d_{in}}$, i.e. the sum of the word embeddings.

Alternatively, we can use a convolutional method: as in \cite{kim2014convolutional}, we perform a convolution over each window of words in the sentence. We use max-over-time pooling to obtain a fixed-dimensional sentence representation in $\mathbb{R}^{d_f}$ where $d_f$ is the number of filters.

% include convolution equations?

The final attention computed for word $j$ in sentence $i$ will thus be
$$\alpha_{i,j} = \alpha_i \cdot \widetilde{\alpha}_{i,j}$$
following the interpretation of $\alpha_{i,j}$ as the probability mass on $w_{i,j}$.

We call this method of attention \emph{Model 2}.

\section{Model 3: Coarse-to-Fine Sparse Attention}

With hierarchical attention, we still do not obtain significant gains in efficiency, since we have to compute RNN states for all words in the source document. Therefore, the idea that underlies this project is to apply stochastic sampling methods to the attention distribution $\alpha$.

Specifically, rather than computing the context $\widetilde{c} = \sum_i \alpha_i h_i$, we can sample from the probability distribution $\alpha_i$ to obtain a single state $h_i$, and we set $\widetilde{c} = h_i$ as the sampled hidden state.

Known in the literature as ``hard attention'' \citep{xu2015captioning}, this model loses the property of being end-to-end differentiable and thus cannot be trained with standard backpropagation. However, reinforcement learning provides a way to circumvent this issue, as described below.

We take Model 2 and apply hard attention at the sentence level, but keep the word level attention per sentence as is. That is, we sample from the attention weights $\alpha_1, \ldots, \alpha_m$ to obtain a one-hot encoding for the sentence attention, and apply the same multiplication with this one-hot vector on the word-level attention weights $\widetilde{\alpha}_{i,1}, \ldots, \widetilde{\alpha}_{i,n_i}$. We call this \emph{Model 3}.

% better notation needed: perhaps \alpha^{sent}?

% more needed here. sparsemax

% write the factorization assumption

\subsection{Multiple Samples}

From our initial experiments with Model 3, we found that taking a single sample was not very effective. However, we discovered that sampling multiple times from the distribution $\alpha$ significantly improves performance.

We sample based on the multinomial distribution $\mathrm{Mult}(k, \{\alpha_i\}_{i=1}^n)$ to produce the sentence-level attention vector $\alpha$ of length $n$, with $\alpha_i = x_i / k$, where $x_i$ is the number of times index $i$ was sampled. $k$ is a hyperparameter which can be tuned, and we found that $k=5$ works well in our experiments.

The intuition here is for the hard attention model to more closely approximate the soft attention model, as it can select more sentences to produce the context.

\subsection{Curriculum}

Since training using policy gradients tends to be noisy and slow to converge, we experimented with a curriculum that starts training with soft attention and in epoch $t$, trains a minibatch using hard attention with probability $p_t = 1 - 1/\sqrt{t}$ \citep{bengio2016hardntm}.

While we found this to be helpful for single sample hard attention, it was not necessary for effective training with multisampled hard attention. We prefer to train solely with hard attention when possible, as we are able to save computation at training time.

\section{Sparsemax}

% describe


\chapter{Experiments}

\section{Data}
% if we do other experiments, include more here.

\subsection{Yuntian's stuff}

% need table

\subsection{CNN/Dailymail}

Experiments were performed on the CNN/Dailymail dataset from \cite{hermann2015read}. While the dataset was created for a question-answering task, the dataset format is suited for summary. Each data point is a news document accompanied by up to 4 ``highlights'', and we take the first of these as our target summary. Train, validation, and test splits are provided along with document tokenization and sentence splitting. We do additional preprocessing by replacing all numbers with \# and appending end of sentence tokens to each sentence. We limit our vocabulary size to 50000 most frequent words, replacing the rest with \texttt{<unk>} tokens. We dropped the documents which had an empty source (which came from photo articles).

 Table~\ref{data_stats} lists statistics for the CNN/Dailymail dataset.

% dataset statistics
\begin{table}[h]
\centering
\begin{tabular}{lcc}
\toprule
Dataset  & CNN & Dailymail \\
\midrule
Train size & 90267 & 196962 \\
Valid size & 1221 & 12149 \\
Average words per doc & 794 & 832\\
Average sents per doc & 21 & 29\\
Average words per sent & 36 & 27\\
Average words per summary & 13 & 15 \\
\bottomrule
\end{tabular}
\caption{Statistics for CNN/Dailymail data.}
\label{data_stats}
\end{table}

% examples of data needed


% is it even worth mentioning this?
%\cite{Chen2016} argue that the CNN/Dailymail is not the best dataset for 

\section{Synthetic Pretraining} % are we still doing this???

We found that unsupervised pretraining on the given dataset is beneficial to learning. For each document, we randomly sample 2 sentences and concatenate to form the target sentence in a new synthetic dataset. We can sample multiple times to have multiple targets for a given source document, and we found that 5 samples was most beneficial to learning (performance drops with significantly more samples). We then train on the synthetic dataset for 5 epochs and initialize future training with the learned weights.

\section{Implementation Details}

A few implementation details were necessary to make minibatch training possible. First, instead of taking attention over each individual sentence, we arrange the first 400 words of the document into a 10 by 40 image, and take each row to be a sentence. Second, we pad short documents to the maximum length with a special padding word, and allow the model to attend to it. However, we zero out word embeddings for the padding states and also zero out their corresponding LSTM states. We found in practice that very little of the attention ended up on the padding words.

Ideally, we would prefer to not truncate documents, especially since later context can be important for summarizing the document. Due to memory issues, this is a problem we still have to resolve.

\section{Models}

\subsection{Baselines}

For a baseline, we take the first 15 words of the document (chosen as the average length of a sentence in the training dataset). We call this \textsc{First}. 

% Need way more here...
% ideas: berkeley system (if it works), look for others - sentence fusion?


\subsection{Our models}

We ran experiments with Models 0 to 3 as described above. Model 0 serves as the baseline.

\begin{itemize}
\item Model 0: Soft attention.
\item Model 1: Hierarchical LSTM, soft attention over all.
\item Model 2: Hierarchical LSTM, soft hierarchical attention.
\item Model 3: Hierarchical LSTM, hard attention over sentences.
\item Model 3+multisampling: We include multisampling with $k=5$.
\end{itemize}

%% Help so bad!!!


\section{Training}

%% fix batch size and other details, and also note we train until convergence (which is slightly longer for hard attn)
We train with minibatch stochastic gradient descent (SGD) with batch size 32 for 13 epochs, renormalizing gradients to be below norm 5. We initialize the learning rate to 1, and begin decaying it by 0.5 each epoch after the validation perplexity stops decreasing. 

We use 2 layer LSTMs with 500 hidden units, and we initialize word embeddings with 300-dimensional word2vec embeddings \citep{mikolov2013word2vec}. For convolutional layers, we use a kernel width of 6 and 600 filters.

%Rewards for multisampling are scaled by a factor of $0.04$ (in addition to scaling by the moving variance).

In the next chapter we show results.

\chapter{Results}

\section{Evaluation} % other experiments? find other metrics
% also, where should we talk about the fact that ROUGE sucks?

We report metrics for best validation perplexity and ROUGE scores \citep{lin2004rouge}. We use the ROUGE balanced F-scores with ROUGE-1 (unigrams), ROUGE-2 (bigrams), and ROUGE-L (longest common substring). We chose F-scores since recall is biased towards longer sentences.

To generate summaries for evaluation, we run beam search with a beam size of 5.



\begin{table}[h]
\centering
\begin{tabular}{llcccc}
 \toprule
 Model &  & PPL & ROUGE-1 & ROUGE-2 & ROUGE-L \\
 \midrule
\textsc{First} & & - & 23.1 & 9.8 & 20.5 \\
\textsc{berkeley} \\
\textsc{Model 0} & &  \\
 \textsc{Model 1} & & \\
 \textsc{Model 2} & & 16.2 & 24.5 & 12.0 & 22.9 \\
\textsc{Model 2+SynthPre} & & 16.0 & 23.7 & 11.5 & 22.0 \\
 \textsc{Model 3} & &   \\
 \textsc{Model 3+sample5} & & 17 & 23.9 & 11.3 & 22.4\\
\textsc{Model 3+sample5+SynthPre} & & 14.6 & 24.1 & 11.7 & 22.5\\
 \bottomrule
\end{tabular}
\caption{Summarization results for CNN/Dailymail.}
\label{table:summary}
\end{table}

\section{Analysis}

See Table~\ref{table:summary} for summary results.

%% attention figures needed here

We notice that Model 2 has the best performance, while multisampling is comparable. We hypothesize that by sampling multiple times, the model learns to approximate the soft attention distribution 

\chapter{Discussion}


\chapter{Conclusion}


\bibliographystyle{plainnat}
\bibliography{../papers/Mendeley/library}

\appendix

\section{Attention Visualizations}

hi

\end{document}
